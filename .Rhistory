) +
theme_minimal(base_size = 13)
recall_logit <- results_logit_smote$recall[results_logit_smote$cutoff == 0.5]
f1_logit     <- results_logit_smote$f1[results_logit_smote$cutoff == 0.5]
recall_rf  <- rf_recall
f1_rf      <- rf_f1
recall_svm <- svm_recall
f1_svm     <- svm_f1
recall_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "recall") %>% pull(.estimate)
f1_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "f_meas") %>% pull(.estimate)
recall_tree <- recall
f1_tree     <- f1
recall_pruned <- recall_p
f1_pruned     <- f1_p
# ---- Combine into a single dataframe ----
model_compare <- data.frame(
Model   = c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN", "Decision Tree", "Pruned Tree"),
Recall  = c(recall_logit, recall_rf, recall_svm, recall_knn, recall_tree, recall_pruned),
F1      = c(f1_logit, f1_rf, f1_svm, f1_knn, f1_tree, f1_pruned)
)
knitr::kable(model_compare, digits = 4)
library(tidyverse)
# Long format for ggplot
compare_long <- model_compare %>%
pivot_longer(cols = c("Recall", "F1"),
names_to = "Metric",
values_to = "Score")
ggplot(compare_long, aes(x = Model, y = Score, fill = Metric)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("steelblue", "orange")) +
labs(
title = "Recall and F1 Score Comparison Across Models",
x = "",
y = "Score"
) +
theme_minimal(base_size = 13)
recall_logit <- results_logit_smote$recall[results_logit_smote$cutoff == 0.5]
f1_logit     <- results_logit_smote$f1[results_logit_smote$cutoff == 0.5]
recall_rf  <- rf_recall
f1_rf      <- rf_f1
recall_svm <- svm_recall
f1_svm     <- svm_f1
recall_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "recall") %>% pull(.estimate)
f1_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "f_meas") %>% pull(.estimate)
recall_tree <- recall
f1_tree     <- f1
recall_pruned <- recall_p
f1_pruned     <- f1_p
# ---- Combine into a single dataframe ----
model_compare <- data.frame(
Model   = c("Logistic", "Random Forest", "SVM", "KNN", "Decision Tree", "Pruned Tree"),
Recall  = c(recall_logit, recall_rf, recall_svm, recall_knn, recall_tree, recall_pruned),
F1      = c(f1_logit, f1_rf, f1_svm, f1_knn, f1_tree, f1_pruned)
)
knitr::kable(model_compare, digits = 4)
library(tidyverse)
# Long format for ggplot
compare_long <- model_compare %>%
pivot_longer(cols = c("Recall", "F1"),
names_to = "Metric",
values_to = "Score")
ggplot(compare_long, aes(x = Model, y = Score, fill = Metric)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("steelblue", "orange")) +
labs(
title = "Recall and F1 Score Comparison Across Models",
x = "",
y = "Score"
) +
theme_minimal(base_size = 13)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(test_2019_logit$Wildfire,
test_2019_logit$prob_yes_smote,
levels = c("No","Yes"))
roc_rf <- roc(test_2019$Wildfire, rf_prob, levels = c("No","Yes"))
roc_svm <- roc(test_2019$Wildfire, svm_prob, levels = c("No","Yes"))
roc_knn <- roc(fire_prob_smote$Wildfire, fire_prob_smote$.pred_Yes, levels = c("No","Yes"))
roc_tree <- roc(test_2019$Wildfire,
test_2019$tree_prob_yes,
levels = c("No","Yes"))
tree_pruned_prob <- predict(tree_pruned, test_2019, type="prob")[,"Yes"]
roc_tree_pruned <- roc(test_2019$Wildfire,
tree_pruned_prob,
levels = c("No","Yes"))
plot(roc_logit, col="red", lwd=2,
main="ROC Comparison – All Models")
plot(roc_rf, col="blue", lwd=2, add=TRUE)
plot(roc_svm, col="green", lwd=2, add=TRUE)
plot(roc_knn, col="orange", lwd=2, add=TRUE)
plot(roc_tree, col="purple", lwd=2, add=TRUE)
plot(roc_tree_pruned, col="brown", lwd=2, add=TRUE)
legend("bottomright",
legend=c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN",
"Decision Tree", "Pruned Tree"),
col=c("red","blue","green","orange","purple","brown"),
lwd=2)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(test_2019_logit$Wildfire,
test_2019_logit$prob_yes_smote,
levels = c("No","Yes"))
roc_rf <- roc(test_2019$Wildfire, rf_prob, levels = c("No","Yes"))
roc_svm <- roc(test_2019$Wildfire, svm_prob, levels = c("No","Yes"))
roc_knn <- roc(fire_prob_smote$Wildfire, fire_prob_smote$.pred_Yes, levels = c("No","Yes"))
roc_tree <- roc(test_2019$Wildfire,
test_2019$tree_prob_yes,
levels = c("No","Yes"))
tree_pruned_prob <- predict(tree_pruned, test_2019, type="prob")[,"Yes"]
roc_tree_pruned <- roc(test_2019$Wildfire,
tree_pruned_prob,
levels = c("No","Yes"))
plot(roc_logit, col="red", lwd=2,
main="ROC Comparison – All Models")
plot(roc_rf, col="blue", lwd=2, add=TRUE)
plot(roc_svm, col="green", lwd=2, add=TRUE)
plot(roc_knn, col="orange", lwd=2, add=TRUE)
plot(roc_tree, col="purple", lwd=2, add=TRUE)
plot(roc_tree_pruned, col="brown", lwd=2, add=TRUE)
legend("bottomright",
legend=c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN",
"Decision Tree", "Pruned Tree"),
col=c("red","blue","green","orange","purple","brown"),
lwd=2)
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)
knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
mutate(Wildfire=trimws(Wildfire))%>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)
knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
num_vars <- data[, c("pr","rmax","rmin","sph","srad","tmmn")]
corr_mat <- cor(num_vars, method = "pearson")
corrplot(corr_mat, method = "number", type="upper",
tl.col="black", title="Correlation Matrix (Climate Variables)")
model = data %>%
mutate(Wildfire = trimws(Wildfire),
Wildfire = factor(Wildfire, levels = c('No','Yes')))%>%
tidyr::drop_na()
table(data$Wildfire)
table(data$Year)
train_2018 = model %>% filter(Year==2018) %>% select(-Year)
test_2019 = model %>% filter(Year==2019) %>% select(-Year)
smote = function(df,target='Wildfire',K=5,seed=2){
df = as.data.frame(df)
y = df[[target]]
X = df[,setdiff(names(df), target), drop = FALSE]
y_num = ifelse(y =='Yes',1,0)
set.seed(seed)
sm = smotefamily::SMOTE(X, y_num, K=K)
out = sm$data
colnames(out)[ncol(out)] = 'y_num'
out[[target]] = factor(
ifelse(out$y_num == 1, 'Yes','No'),
levels = c('No','Yes')
)
out$y_num = NULL
out = out[,c(setdiff(names(out), target), target)]
return(out)
}
train_smote = smote(train_2018, target='Wildfire', K=5, seed=2)
test_smote = smote(test_2019, target='Wildfire',K=5,seed=2)
table(train_smote$Wildfire)
table(test_smote$Wildfire)
fire_recipe_knn = recipe(Wildfire~., data = train_smote) %>% step_normalize(all_numeric_predictors())
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")
knn_model
knn_wf <- workflow() %>%
add_recipe(fire_recipe_knn) %>%
add_model(knn_model)
knn_wf
knn_fit <- fit(knn_wf, data = train_smote)
install.packages("kknn")
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)
knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
num_vars <- data[, c("pr","rmax","rmin","sph","srad","tmmn")]
corr_mat <- cor(num_vars, method = "pearson")
corrplot(corr_mat, method = "number", type="upper",
tl.col="black", title="Correlation Matrix (Climate Variables)")
model = data %>%
mutate(Wildfire = trimws(Wildfire),
Wildfire = factor(Wildfire, levels = c('No','Yes')))%>%
tidyr::drop_na()
table(data$Wildfire)
table(data$Year)
train_2018 = model %>% filter(Year==2018) %>% select(-Year)
test_2019 = model %>% filter(Year==2019) %>% select(-Year)
smote = function(df,target='Wildfire',K=5,seed=2){
df = as.data.frame(df)
y = df[[target]]
X = df[,setdiff(names(df), target), drop = FALSE]
y_num = ifelse(y =='Yes',1,0)
set.seed(seed)
sm = smotefamily::SMOTE(X, y_num, K=K)
out = sm$data
colnames(out)[ncol(out)] = 'y_num'
out[[target]] = factor(
ifelse(out$y_num == 1, 'Yes','No'),
levels = c('No','Yes')
)
out$y_num = NULL
out = out[,c(setdiff(names(out), target), target)]
return(out)
}
train_smote = smote(train_2018, target='Wildfire', K=5, seed=2)
test_smote = smote(test_2019, target='Wildfire',K=5,seed=2)
table(train_smote$Wildfire)
table(test_smote$Wildfire)
fire_recipe_knn = recipe(Wildfire~., data = train_smote) %>% step_normalize(all_numeric_predictors())
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")
knn_model
knn_wf <- workflow() %>%
add_recipe(fire_recipe_knn) %>%
add_model(knn_model)
knn_wf
knn_fit <- fit(knn_wf, data = train_smote)
knn_predictions <- predict(knn_fit, test_smote) %>%
bind_cols(test_smote)
head(knn_predictions)
conf_mat(
knn_predictions,
truth = Wildfire,
estimate = .pred_class
)
knn_metrics = yardstick::metric_set(
yardstick::accuracy,
yardstick::precision,
yardstick::recall,
yardstick::f_meas
)
knn_metrics(
knn_predictions,
truth = Wildfire,
estimate = .pred_class,
event_level = 'second'
)
metric_set(kap)(
knn_predictions,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"   # "Yes" (Wildfire) is the positive class
)
# Get predicted probabilities for both classes
knn_prob <- predict(knn_fit, test_smote, type = "prob") %>%
bind_cols(test_smote)
# Create ROC object
roc_obj <- roc_curve(
knn_prob,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
# Plot ROC curve using ggplot
autoplot(roc_obj) +
ggtitle("ROC Curve for KNN Wildfire Prediction (2019)") +
theme_minimal()
svm_model <- svm(
Wildfire ~ .,
data = train_smote,
kernel = "radial",
probability = TRUE,
scale = TRUE
)
# Predictions
svm_pred <- predict(svm_model, test_smote)
svm_cm <- table(Predicted = svm_pred, Actual = test_smote$Wildfire)
svm_cm
conf_mat(
svm_pred,
truth = Wildfire,
estimate = .pred_class
)
# Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)
# Metrics
TP_svm <- svm_cm["Yes", "Yes"]
FN_svm <- svm_cm["No",  "Yes"]
FP_svm <- svm_cm["Yes", "No"]
svm_precision <- TP_svm / (TP_svm + FP_svm)
svm_recall    <- TP_svm / (TP_svm + FN_svm)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
c(
accuracy  = svm_accuracy,
precision = svm_precision,
recall    = svm_recall,
F1        = svm_f1
)
# Probability prediction
svm_prob <- attr(
predict(svm_model, test_smote, probability = TRUE),
"probabilities"
)[, "Yes"]
# ROCR performance objects
pred_svm <- prediction(svm_prob, test_smote$Wildfire)
perf_svm <- performance(pred_svm, "tpr", "fpr")
# Plot ROC curve
plot(perf_svm, col = "red", lwd = 2, main = "SVM ROC Curve")
abline(0, 1, lty = 2, col = "gray")
# Compute AUC
svm_auc <- performance(pred_svm, "auc")@y.values[[1]]
svm_auc
# Random Forest on SMOTE balanced data
rf_model <- randomForest(
Wildfire ~ .,
data = train_smote,
ntree = 500,
importance = TRUE
)
# Predictions
rf_pred <- predict(rf_model, test_smote)
rf_cm <- table(Predicted = rf_pred, Actual = test_smote$Wildfire)
rf_cm
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(accuracy = rf_accuracy,
precision = rf_precision,
recall    = rf_recall,
F1        = rf_f1)
# ROC curve
rf_prob <- predict(rf_model, newdata=test_smote, type = "prob")[,"Yes"]
pred <- prediction(rf_prob, test_smote$Wildfire)
perf <- performance(pred, "tpr", "fpr")
# AUC
auc_perf <- performance(pred, "auc")
rf_auc <- auc_perf@y.values[[1]]
rf_auc
plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve")
abline(0, 1, lty = 2, col = "gray")
