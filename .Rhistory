# 2. Split into train (2018) and test (2019)
train2018 <- subset(data_clean, Year == 2018, select = -Year)
test2019  <- subset(data_clean, Year == 2019, select = -Year)
data_clean$Wildfire <- factor(data_clean$Wildfire, levels = c("No", "Yes"))
str(data_clean$Wildfire)
table(data_clean$Wildfire)
nrow(train2018)
nrow(test2019)
print(colnames(data_clean))
table(data_clean$Wildfire, useNA = "ifany")
table(train2018$Wildfire)
colSums(is.na(train2018[train2018$Wildfire == "Yes", ]))
num_vars <- train2018[, c("pr","rmax","rmin","sph","srad","tmmn")]
corr_mat <- cor(num_vars, method = "pearson")
corrplot(corr_mat, method = "number", type="upper",
tl.col="black", title="Correlation Matrix (Climate Variables)")
yes_cases <- train2018 %>% filter(Wildfire == "Yes")
no_cases  <- train2018 %>% filter(Wildfire == "No")
set.seed(123)
upsampled_yes <- yes_cases %>% sample_n(nrow(no_cases), replace = TRUE)
train_balanced <- bind_rows(no_cases, upsampled_yes)
table(train2018$Wildfire)
table(train_balanced$Wildfire)   # now balanced
rf_model <- randomForest(
Wildfire ~ .,
data = train_balanced,
ntree = 500,
importance = TRUE
)
rf_pred <- predict(rf_model, test2019)
rf_cm <- table(Predicted = rf_pred, Actual = test2019$Wildfire)
rf_cm
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
rf_accuracy
TP <- rf_cm["Yes","Yes"]
FN <- rf_cm["No","Yes"]
FP <- rf_cm["Yes","No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(precision = rf_precision, recall = rf_recall, F1 = rf_f1)
rf_prob <- predict(rf_model, test2019, type = "prob")[,"Yes"]
pred <- prediction(rf_prob, test2019$Wildfire)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col="blue", lwd=2, main="Random Forest ROC Curve")
abline(0,1,lty=2,col="gray")
# Numeric importance values
importance(rf_model)
# Plot graph
varImpPlot(rf_model, main = "Random Forest Feature Importance")
# Convert RF importance to a data frame
imp <- importance(rf_model)
imp_df <- data.frame(
Feature = rownames(imp),
Importance = imp[, "MeanDecreaseGini"]   # Change if you prefer MeanDecreaseAccuracy
)
# Plot all 20 features (no top_n needed)
imp_df %>%
ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +
geom_col(fill = "lightblue") +
coord_flip() +
labs(
title = "Feature Importance for Wildfire Prediction",
x = "Feature",
y = "Mean Decrease in Gini (Variable Importance)"
) +
theme_minimal(base_size = 13)
# 2) Random Forest on SMOTE balanced data
set.seed(123)
rf_model <- randomForest(
Wildfire ~ .,
data = train_balanced,
ntree = 500,
importance = TRUE
)
# Predictions
rf_pred <- predict(rf_model, test2019)
rf_cm <- table(Predicted = rf_pred, Actual = test2019$Wildfire)
rf_cm
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(accuracy = rf_accuracy,
precision = rf_precision,
recall    = rf_recall,
F1        = rf_f1)
# ROC curve
rf_prob <- predict(rf_model, test2019, type = "prob")[, "Yes"]
pred <- prediction(rf_prob, test2019$Wildfire)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve (SMOTE)")
abline(0, 1, lty = 2, col = "gray")
# Variable importance
importance(rf_model)
varImpPlot(rf_model, main = "Random Forest Feature Importance (SMOTE)")
imp <- importance(rf_model)
imp_df <- data.frame(
Feature    = rownames(imp),
Importance = imp[, "MeanDecreaseGini"]
)
imp_df %>%
ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +
geom_col() +
coord_flip() +
labs(
title = "Feature Importance for Wildfire Prediction (RF + SMOTE)",
x = "Feature",
y = "Mean Decrease in Gini"
) +
theme_minimal(base_size = 13)
train2018$Year <- NULL
test2019$Year <- NULL
svm_model <- svm(
Wildfire ~ .,
data = train2018,
kernel = "radial",
probability = TRUE,
scale = TRUE,
class.weights = c("No" = 1, "Yes" = 50)
)
# 4. Predictions
svm_pred <- predict(svm_model, test2019)
# 5. Confusion matrix
svm_cm <- table(Predicted = svm_pred, Actual = test2019$Wildfire)
svm_cm
# 6. Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)
svm_accuracy
# Precision, Recall, F1
TP <- svm_cm["Yes", "Yes"]
FN <- svm_cm["No", "Yes"]
FP <- svm_cm["Yes", "No"]
svm_precision <- TP / (TP + FP)
svm_recall    <- TP / (TP + FN)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
c(
precision = svm_precision,
recall    = svm_recall,
F1        = svm_f1
)
# ---- ROC & AUC ----
# Probability predictions
svm_prob <- attr(
predict(svm_model, test2019, probability = TRUE),
"probabilities"
)[, "Yes"]
# ROCR performance objects
pred_svm <- prediction(svm_prob, test2019$Wildfire)
perf_svm <- performance(pred_svm, "tpr", "fpr")
# Plot ROC curve
plot(perf_svm, col = "red", lwd = 2, main = "SVM ROC Curve")
abline(0, 1, lty = 2, col = "gray")
# AUC
svm_auc <- performance(pred_svm, "auc")@y.values[[1]]
svm_auc
# SVM on SMOTE balanced data
svm_model <- svm(
Wildfire ~ .,
data = train_balanced,   # SMOTE data
kernel = "radial",
probability = TRUE,
scale = TRUE
)
# Predictions
svm_pred <- predict(svm_model, test2019)
svm_cm <- table(Predicted = svm_pred, Actual = test2019$Wildfire)
svm_cm
# Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)
# Metrics
TP_svm <- svm_cm["Yes", "Yes"]
FN_svm <- svm_cm["No",  "Yes"]
FP_svm <- svm_cm["Yes", "No"]
svm_precision <- TP_svm / (TP_svm + FP_svm)
svm_recall    <- TP_svm / (TP_svm + FN_svm)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
c(
accuracy  = svm_accuracy,
precision = svm_precision,
recall    = svm_recall,
F1        = svm_f1
)
options(repos = c(CRAN = "https://cloud.r-project.org"))
required_packages <- c(
"sf", "ggplot2", "tidyverse",
"tidymodels", "themis"
)
new_packages <- required_packages[
!(required_packages %in% installed.packages()[, "Package"])
]
if (length(new_packages)) {
install.packages(new_packages)
}
library(sf)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(themis)
# Some of common RMD options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
fire_all <- read_csv("Fire_Incidence_Data 2018-2019.csv")
glimpse(fire_all)
table(fire_all$Year)
fire_model <- fire_all %>%
select(
Wildfire,     # TARGET (Yes/No)
Year,
latitude,
longitude,
pr,
rmax,
rmin,
sph,
srad,
tmmn
) %>%
drop_na()
glimpse(fire_model)
fire_model <- fire_model %>%
mutate(Wildfire = as.factor(Wildfire))
levels(fire_model$Wildfire)
fire_train <- fire_model %>%
filter(Year == 2018)
fire_test <- fire_model %>%
filter(Year == 2019)
nrow(fire_train)
nrow(fire_test)
fire_recipe <- recipe(
Wildfire ~ .,
data = fire_train
) %>%
step_rm(Year) %>%
step_normalize(all_numeric_predictors()) %>%
step_upsample(Wildfire)
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")
knn_model
fire_wf <- workflow() %>%
add_recipe(fire_recipe) %>%
add_model(knn_model)
fire_wf
knn_fit <- fit(fire_wf, data = fire_train)
install.packges('kknn')
install.packages('kknn')
library(kknn)
options(repos = c(CRAN = "https://cloud.r-project.org"))
required_packages <- c(
"sf", "ggplot2", "tidyverse",
"tidymodels", "themis"
)
new_packages <- required_packages[
!(required_packages %in% installed.packages()[, "Package"])
]
if (length(new_packages)) {
install.packages(new_packages)
}
library(sf)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(themis)
library(kknn)
# Some of common RMD options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
fire_all <- read_csv("Fire_Incidence_Data 2018-2019.csv")
glimpse(fire_all)
table(fire_all$Year)
fire_model <- fire_all %>%
select(
Wildfire,     # TARGET (Yes/No)
Year,
latitude,
longitude,
pr,
rmax,
rmin,
sph,
srad,
tmmn
) %>%
drop_na()
glimpse(fire_model)
fire_model <- fire_model %>%
mutate(Wildfire = as.factor(Wildfire))
levels(fire_model$Wildfire)
fire_train <- fire_model %>%
filter(Year == 2018)
fire_test <- fire_model %>%
filter(Year == 2019)
nrow(fire_train)
nrow(fire_test)
fire_recipe <- recipe(
Wildfire ~ .,
data = fire_train
) %>%
step_rm(Year) %>%
step_normalize(all_numeric_predictors()) %>%
step_upsample(Wildfire)
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")
knn_model
fire_wf <- workflow() %>%
add_recipe(fire_recipe) %>%
add_model(knn_model)
fire_wf
knn_fit <- fit(fire_wf, data = fire_train)
fire_predictions <- predict(knn_fit, fire_test) %>%
bind_cols(fire_test)
head(fire_predictions)
conf_mat(
fire_predictions,
truth = Wildfire,
estimate = .pred_class
)
metric_set(accuracy, precision, recall, f_meas)(
fire_predictions,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"   # âœ… This makes "Yes" (Wildfire) the positive class
)
rlang::last_trace()
install.packages('yardstick')
library('yardstick')
options(repos = c(CRAN = "https://cloud.r-project.org"))
required_packages <- c(
"sf", "ggplot2", "tidyverse",
"tidymodels", "themis"
)
new_packages <- required_packages[
!(required_packages %in% installed.packages()[, "Package"])
]
if (length(new_packages)) {
install.packages(new_packages)
}
library(sf)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(themis)
library(kknn)
library(yardstick)
# Some of common RMD options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
fire_all <- read_csv("Fire_Incidence_Data 2018-2019.csv")
glimpse(fire_all)
table(fire_all$Year)
fire_model <- fire_all %>%
select(
Wildfire,     # TARGET (Yes/No)
Year,
latitude,
longitude,
pr,
rmax,
rmin,
sph,
srad,
tmmn
) %>%
drop_na()
glimpse(fire_model)
fire_model <- fire_model %>%
mutate(Wildfire = as.factor(Wildfire))
levels(fire_model$Wildfire)
fire_train <- fire_model %>%
filter(Year == 2018)
fire_test <- fire_model %>%
filter(Year == 2019)
nrow(fire_train)
nrow(fire_test)
fire_recipe <- recipe(
Wildfire ~ .,
data = fire_train
) %>%
step_rm(Year) %>%
step_normalize(all_numeric_predictors()) %>%
step_upsample(Wildfire)
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")
knn_model
fire_wf <- workflow() %>%
add_recipe(fire_recipe) %>%
add_model(knn_model)
fire_wf
knn_fit <- fit(fire_wf, data = fire_train)
fire_predictions <- predict(knn_fit, fire_test) %>%
bind_cols(fire_test)
head(fire_predictions)
conf_mat(
fire_predictions,
truth = Wildfire,
estimate = .pred_class
)
knn_metrics = yardstick::metric_set(
yardstick::accuracy,
yardstick::precision,
yardstick::recall,
yardstick::f_meas
)
knn_metrics(
fire_predictions
truth = Wildfire,
knn_metrics = yardstick::metric_set(
yardstick::accuracy,
yardstick::precision,
yardstick::recall,
yardstick::f_meas
)
knn_metrics(
fire_predictions,
truth = Wildfire,
estimate = .pred_class,
event_level = 'second'
)
metric_set(kap)(
fire_predictions,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"   # "Yes" (Wildfire) is the positive class
)
# Get predicted probabilities for both classes
fire_prob <- predict(knn_fit, fire_test, type = "prob") %>%
bind_cols(fire_test)
# Check the probability column names
colnames(fire_prob)
# Create ROC object
roc_obj <- roc_curve(
fire_prob,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
# Plot ROC curve using ggplot
autoplot(roc_obj) +
ggtitle("ROC Curve for KNN Wildfire Prediction (2019)") +
theme_minimal()
# knn + SMOTE
```{r}
fire_recipe_smote <- recipe(
Wildfire ~ .,
data = fire_train
) %>%
step_rm(Year) %>%
step_normalize(all_numeric_predictors()) %>%
step_smote(Wildfire)
fire_recipe_smote
prep_smote <- prep(fire_recipe_smote)
baked_smote <- bake(prep_smote, new_data = NULL)
table(baked_smote$Wildfire)
fire_wf_smote <- workflow() %>%
add_recipe(fire_recipe_smote) %>%
add_model(knn_model)
fire_wf_smote
knn_fit_smote <- fit(fire_wf_smote, data = fire_train)
fire_predictions_smote <- predict(knn_fit_smote, fire_test) %>%
bind_cols(fire_test)
head(fire_predictions_smote)
conf_mat(
fire_predictions_smote,
truth   = Wildfire,
estimate = .pred_class
)
knn_metrics(
fire_predictions_smote,
truth    = Wildfire,
estimate = .pred_class,
event_level = "second"
)
fire_prob_smote <- predict(knn_fit_smote, fire_test, type = "prob") %>%
bind_cols(fire_test)
colnames(fire_prob_smote)
roc_obj_smote <- roc_curve(
fire_prob_smote,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
autoplot(roc_obj_smote) +
ggtitle("ROC Curve for KNN with SMOTE Wildfire Prediction (2019)") +
theme_minimal()
