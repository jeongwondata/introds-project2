.pred_Yes,
event_level = "second"
)
# Plot ROC curve using ggplot
autoplot(roc_obj) +
ggtitle("ROC Curve for KNN Wildfire Prediction (2019)") +
theme_minimal()
svm_model <- svm(
Wildfire ~ .,
data = train_smote,
kernel = "radial",
probability = TRUE,
scale = TRUE
)
# Predictions
svm_pred <- predict(svm_model, test_smote)
svm_cm <- table(Predicted = svm_pred, Actual = test_smote$Wildfire)
svm_cm
conf_mat(
svm_pred,
truth = Wildfire,
estimate = .pred_class
)
svm_predictions <- predict(svm_fit, fire_test) %>%
bind_cols(fire_test)
svm_predictions <- predict(svm_model, test_2019) %>%
bind_cols(fire_test)
conf_mat(
svm_pred,
truth = Wildfire,
estimate = .pred_class
)
svm_predictions <- predict(svm_model, test_2019) %>%
bind_cols(test_2019)
conf_mat(
svm_pred,
truth = Wildfire,
estimate = .pred_class
)
conf_mat(
svm_predictions,
truth = Wildfire,
estimate = .pred_class
)
conf_mat(
svm_predictions,
truth = Wildfire,
estimate = .pred_Yes
)
conf_mat(
svm_predictions,
truth = Wildfire,
estimate = .pred_class,
levels='second'
)
svm_predictions <- test_2019 %>%
mutate(.pred_class = predict(svm_model, test_2019))
conf_mat(svm_predictions, truth = Wildfire, estimate = .pred_class)
library(caret)
svm_pred_2019 <- predict(svm_model, test_2019)
confusionMatrix(
data = svm_pred_2019,
reference = test_2019$Wildfire,
positive = "Yes"
)
# Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)
# Metrics
TP_svm <- svm_cm["Yes", "Yes"]
FN_svm <- svm_cm["No",  "Yes"]
FP_svm <- svm_cm["Yes", "No"]
svm_precision <- TP_svm / (TP_svm + FP_svm)
svm_recall    <- TP_svm / (TP_svm + FN_svm)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
c(
accuracy  = svm_accuracy,
precision = svm_precision,
recall    = svm_recall,
F1        = svm_f1
)
# Probability prediction
svm_prob <- attr(
predict(svm_model, test_smote, probability = TRUE),
"probabilities"
)[, "Yes"]
# ROCR performance objects
pred_svm <- prediction(svm_prob, test_smote$Wildfire)
perf_svm <- performance(pred_svm, "tpr", "fpr")
# Plot ROC curve
plot(perf_svm, col = "red", lwd = 2, main = "SVM ROC Curve")
abline(0, 1, lty = 2, col = "gray")
# Compute AUC
svm_auc <- performance(pred_svm, "auc")@y.values[[1]]
svm_auc
# Random Forest on SMOTE balanced data
rf_model <- randomForest(
Wildfire ~ .,
data = train_smote,
ntree = 500,
importance = TRUE
)
# Predictions
rf_pred <- predict(rf_model, test_smote)
rf_cm <- table(Predicted = rf_pred, Actual = test_smote$Wildfire)
rf_cm
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(accuracy = rf_accuracy,
precision = rf_precision,
recall    = rf_recall,
F1        = rf_f1)
# ROC curve
rf_prob <- predict(rf_model, newdata=test_smote, type = "prob")[,"Yes"]
pred <- prediction(rf_prob, test_smote$Wildfire)
perf <- performance(pred, "tpr", "fpr")
# AUC
auc_perf <- performance(pred, "auc")
rf_auc <- auc_perf@y.values[[1]]
rf_auc
plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve")
abline(0, 1, lty = 2, col = "gray")
tree_smote = rpart(Wildfire ~ .,
data = train_smote,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.001))
test_smote$tree_pred_smote = predict(tree_smote,
newdata = test_smote,
type = "class")
cm_smote = table(Predicted = test_smote$tree_pred_smote,
Actual= test_smote$Wildfire)
cm_smote
TP_s = cm_smote["Yes", "Yes"]
FP_s = cm_smote["Yes", "No"]
FN_s = cm_smote["No",  "Yes"]
TN_s = cm_smote["No",  "No"]
accuracy_s  = (TP_s + TN_s) / sum(cm_smote)
precision_s = TP_s / (TP_s + FP_s)
recall_s= TP_s / (TP_s + FN_s)
f1_s <- 2 * precision_s * recall_s / (precision_s + recall_s)
accuracy_s
precision_s
recall_s
f1_s
printcp(tree_smote)
best_cp = tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]
best_cp
tree_pruned = rpart::prune(tree_smote, cp = best_cp)
rpart.plot(tree_pruned,type = 1,extra = 1,cex = 0.4,faclen = 3)
test_smote$tree_pred_pruned <- predict(
tree_pruned,
newdata = test_smote,
type = "class"
)
cm_pruned <- table(
Predicted = test_smote$tree_pred_pruned,
Actual    = test_smote$Wildfire
)
cm_pruned
TP_p <- cm_pruned["Yes", "Yes"]
FP_p <- cm_pruned["Yes", "No"]
FN_p <- cm_pruned["No",  "Yes"]
TN_p <- cm_pruned["No",  "No"]
accuracy_p  <- (TP_p + TN_p) / sum(cm_pruned)
precision_p <- TP_p / (TP_p + FP_p)
recall_p    <- TP_p / (TP_p + FN_p)
f1_p <- 2 * precision_p * recall_p / (precision_p + recall_p)
accuracy_p
precision_p
recall_p
f1_p
tree_prob <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]
tree_prob <- data.frame(
.pred_Yes = tree_prob,
model = "Decision Tree"
)
# Create ROC object
roc_dt <- roc_curve(
tree_prob,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
# Create ROC object
roc_dt <- (Wildfire = test_smote$Wildfire,
roc_dt <- roc_curve(tree_prob,
truth = Wildfire,
.pred_Yes = as.numeric(tree_prob$.pred_Yes),
model = "Decision Tree"
)
roc_dt <- roc_curve(tree_prob,
truth = Wildfire,
.pred_Yes,
event_level = 'second')
# Create ROC object
tree_df <- data.frame(
Wildfire = test_smote$Wildfire,
.pred_Yes = as.numeric(tree_prob)
)
tree_df <- data.frame(
.pred_Yes = tree_prob,
model='Decision Tree'
)
roc_dt <- roc_curve(tree_df,
truth = Wildfire,
.pred_Yes,
event_level = 'second')
tree_prob_yes <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]
tree_prob_yes <- as.numeric(tree_prob_yes)
tree_df <- data.frame(
Wildfire  = test_smote$Wildfire,
.pred_Yes = tree_prob_yes
)
names(tree_df)
roc_dt <- roc_curve(tree_df,
truth = Wildfire,
.pred_Yes,
event_level = 'second')
autoplot(roc_dt) +
ggtitle("ROC Curve for Pruned Decision Tree (2019)") +
theme_minimal()
logit_smote_model <- glm(
Wildfire ~ .,
data   = train_smote,
family = binomial
)
summary(logit_smote_model)
test_smote$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_smote,
type   = "response"
)
eval_cutoff <- function(prob, actual, cutoff) {
pred <- ifelse(prob >= cutoff, "Yes", "No")
pred <- factor(pred, levels = c("No", "Yes"))
cm <- table(Predicted = pred, Actual = actual)
print(cm)
TP_l <- cm["Yes", "Yes"]
FP_l <- cm["Yes", "No"]
FN_l <- cm["No",  "Yes"]
TN_l <- cm["No",  "No"]
accuracy_l  <- (TP_l + TN_l) / sum(cm)
precision_l <- TP_l / (TP_l + FP_l)
recall_l    <- TP_l / (TP_l + FN_l)
f1_l        <- 2 * precision_l * recall_l / (precision_l + recall_l)
out <- c(
cutoff    = cutoff,
accuracy  = accuracy_l,
precision = precision_l,
recall    = recall_l,
f1        = f1_l
)
return(out)
}
cutoffs <- c(0.1, 0.2, 0.3, 0.5)
results_logit_smote <- t(
sapply(cutoffs, function(c) {
eval_cutoff(
prob   = test_smote$prob_yes_smote,
actual = test_smote$Wildfire,
cutoff = c
)
})
)
results_logit_smote <- as.data.frame(results_logit_smote)
results_logit_smote[] <- lapply(results_logit_smote, as.numeric)
knitr::kable(results_logit_smote, digits = 4)
logit_prob <- predict(logit_smote_model, test_smote, type = "response")
logit_prob <- data.frame(
.pred_Yes = logit_prob,
model = "Logistic Regression"
)
# Create ROC object
roc_l <- roc_curve(
logit_prob,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
logit_prob_yes <- predict(logit_smote_model, test_smote, type = "prob")[, "Yes"]
logit_prob_yes <- predict(
logit_smote_model,
newdata = test_smote,
type = "response"
)
logit_df = data.frame(Wildfire = test_somte&Wildfire,
.pred_Yes = as.numeric(logit_prob_yes))
logit_df = data.frame(Wildfire = test_smote&Wildfire,
.pred_Yes = as.numeric(logit_prob_yes))
logit_df = data.frame(Wildfire = test_smote&Wildfire,
.pred_Yes = logit_prob_yes)
logit_df = data.frame(Wildfire = test_smote$Wildfire,
.pred_Yes = logit_prob_yes)
roc_logit <- roc_curve(
logit_prob,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
roc_logit <- roc_curve(
logit_df,
truth = Wildfire,
.pred_Yes,
event_level = "second"
)
# Plot ROC curve using ggplot
autoplot(roc_l) +
ggtitle("ROC Curve for Logistic Regression (2019)") +
theme_minimal()
# Plot ROC curve using ggplot
autoplot(roc_logit) +
ggtitle("ROC Curve for Logistic Regression (2019)") +
theme_minimal()
logit_prob <- predict(logit_smote_model, newdata = test_smote, type = "response")
logit_prob <- as.numeric(logit_prob)
knn_pred_class = predict(knn_fit, test_smote)$.pred_class
svm_pred_class = predict(svm_model, test_smote)
rf_pred_class = predict(rf_model, newdata=test_smote, type='class')
dt_pred_class = predict(tree_pruned, newdata = test_smote, type='class')
logit_pred_class = factor(ifelse(logit_prob >= 0.5, 'Yes','No'), levels = c('No','Yes'))
fix_levels = function(x) factor(x, levels = c('No','Yes'))
truth_fix = fix_levels(test_smote$Wildfire)
knn_pred_class = fix_levels(knn_pred_class)
svm_pred_class = fix_levels(svm_pred_class)
rf_pred_class = fix_levels(rf_pred_class)
dt_pred_class = fix_levels(dt_pred_class)
logit_pred_class = fix_levels(logit_pred_class)
metric_c = yardstick::metric_set(recall,f_meas)
metric_c = yardstick::metric_set(yardstick::recall,yardstick::f_meas)
metrics_df = bind_rows(
tibble(model='KNN', truth = truth_fix, estimate = knn_pred_class),
tibble(model='SVM', truth = truth_fix, estimate = svm_pred_class),
tibble(model='Random Forest', truth = truth_fix, estimate = rf_pred_class),
tibble(model='Decision Tree', truth = truth_fix, estimate = dt_pred_class),
tibble(model='Logistic Regression', truth = truth_fix, estimate = logit_pred_class)
)%>%
group_by(model)%>%
metric_c(truth=truth, estimate = estimate, event_level = 'second')%>%
ungroup()%>%
select(model, .metric,.estimate)
ggplot(metrics_df, aes(x=model, y=.estimate, fill=.metric)) + geom_col(position = position_dodge(width=0.8), width =0.7) + coord_flip()+ labs(
title = 'Comparing Recall and F1 Score Across Models(SMOTE-balanced)',
x='Models',
y='Score',
fil='Metric'
)+
theme_minimal(base_size=14) +
theme(axis.test.x = element_text(angle=25, hjust=1))
rm(roc_df)
gc()
# KNN
knn_df <- tibble(
Wildfire = test_smote$Wildfire,
.pred_Yes = predict(knn_fit, test_smote, type = "prob")$.pred_Yes,
model = "KNN"
)
# SVM
svm_df <- tibble(
Wildfire = test_smote$Wildfire,
.pred_Yes = as.numeric(svm_prob),
model = "SVM"
)
# Random Forest
rf_df <- tibble(
Wildfire = test_smote$Wildfire,
.pred_Yes = as.numeric(rf_prob),
model = "Random Forest"
)
# Decision Tree
tree_df <- tibble(
Wildfire = test_smote$Wildfire,
.pred_Yes = as.numeric(tree_prob$.pred_Yes),
model = "Decision Tree"
)
# Logistic
logit_vec <- if (is.data.frame(logit_prob)) logit_prob[[1]] else logit_prob
logit_df <- tibble(
Wildfire = test_smote$Wildfire,
.pred_Yes = as.numeric(logit_vec),
model = "Logistic Regression"
)
roc_df <- bind_rows(knn_df, svm_df, rf_df, tree_df, logit_df)
roc_data = roc_df%>%
group_by(model)%>%
roc_curve(
truth=Wildfire,
.pred_Yes,
event_level = 'second'
)
ggplot(roc_data, aes(x=1-specificity, y=sensitivity, color=model)) +
geom_line(size=1.2)+
geom_abline(linetype='dashed', color ='gray50') +
labs(
title = 'ROC Curve Comparison Across Models (SMOTE-balanced)',
x='False Positive Rate',
y='True Positive Rate',
color='Model') +
theme_minimal(base_size=14)
# Variable importance
imp_vec <- tree_pruned$variable.importance
imp_df <- data.frame(
Feature = names(imp_vec),
Importance = as.numeric(imp_vec),
row.names = NULL
) %>%
dplyr::arrange(dplyr::desc(Importance))
imp_df %>%
ggplot2::ggplot(ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
ggplot2::geom_col() +
ggplot2::coord_flip() +
ggplot2::labs(
title = "Decision Tree Feature Importance",
x = "Feature",
y = "rpart variable.importance"
) +
ggplot2::theme_minimal(base_size = 13)
# Variable importance
imp_vec <- tree_pruned$variable.importance
imp_df <- data.frame(
Feature = names(imp_vec),
Importance = as.numeric(imp_vec),
row.names = NULL
) %>%
dplyr::arrange(dplyr::desc(Importance))
imp_df %>%
ggplot2::ggplot(ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
ggplot2::geom_col() +
ggplot2::coord_flip() +
ggplot2::labs(
title = "Decision Tree Feature Importance",
x = "Feature",
y = "rpart variable.importance"
) +
ggplot2::theme_minimal(base_size = 13)
# Variable importance
imp_vec <- tree_pruned$variable.importance
imp_df <- data.frame(
Feature = names(imp_vec),
Importance = as.numeric(imp_vec),
row.names = NULL
) %>%
dplyr::arrange(dplyr::desc(Importance))
imp_df %>%
ggplot2::ggplot(ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
ggplot2::geom_col() +
ggplot2::coord_flip() +
ggplot2::labs(
title = "Decision Tree Feature Importance",
x = "Feature",
y = "rpart variable.importance"
) +
ggplot2::theme_minimal(base_size = 13)
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(caret)
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)
knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)
num_vars <- train_2018[,]
corr_mat <- cor(num_vars, method = "pearson")
num_vars <- train_2018%>%dplyr::select(where(is.numeric))
corr_mat <- cor(num_vars, method = "pearson")
corrplot(corr_mat, method = "number", type="upper",
tl.col="black", title="Correlation Matrix (Climate Variables)")
num_vars <- train_2018%>%dplyr::select(where(is.numeric))
corr_mat <- cor(num_vars, method = "pearson")
options(repr.plot.width = 14, repr.plot.height = 12)
corrplot(
corr_mat,
method = "number",
type = "upper",
tl.col = "black",
number.cex = 0.7,
tl.cex = 0.8
)
num_vars <- data%>%dplyr::select(where(is.numeric))
corr_mat <- cor(num_vars, method = "pearson")
options(repr.plot.width = 14, repr.plot.height = 12)
corrplot(
corr_mat,
method = "number",
type = "upper",
tl.col = "black",
number.cex = 0.7,
tl.cex = 0.8
)
