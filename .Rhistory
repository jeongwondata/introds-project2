)
rf_pred <- predict(rf_model, test2019)
rf_cm <- table(Predicted = rf_pred, Actual = test2019$Wildfire)
rf_cm
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
rf_accuracy
TP <- rf_cm["Yes","Yes"]
FN <- rf_cm["No","Yes"]
FP <- rf_cm["Yes","No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(precision = rf_precision, recall = rf_recall, F1 = rf_f1)
rf_prob <- predict(rf_model, test2019, type = "prob")[,"Yes"]
pred <- prediction(rf_prob, test2019$Wildfire)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col="blue", lwd=2, main="Random Forest ROC Curve")
abline(0,1,lty=2,col="gray")
# Numeric importance values
importance(rf_model)
# Plot graph
varImpPlot(rf_model, main = "Random Forest Feature Importance")
# Convert RF importance to a data frame
imp <- importance(rf_model)
imp_df <- data.frame(
Feature = rownames(imp),
Importance = imp[, "MeanDecreaseGini"]   # Change if you prefer MeanDecreaseAccuracy
)
# Plot all 20 features (no top_n needed)
imp_df %>%
ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +
geom_col(fill = "lightblue") +
coord_flip() +
labs(
title = "Feature Importance for Wildfire Prediction",
x = "Feature",
y = "Mean Decrease in Gini (Variable Importance)"
) +
theme_minimal(base_size = 13)
# 2) Random Forest on SMOTE balanced data
set.seed(123)
rf_model <- randomForest(
Wildfire ~ .,
data = train_balanced,
ntree = 500,
importance = TRUE
)
# Predictions
rf_pred <- predict(rf_model, test2019)
rf_cm <- table(Predicted = rf_pred, Actual = test2019$Wildfire)
rf_cm
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)
TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]
rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)
c(accuracy = rf_accuracy,
precision = rf_precision,
recall    = rf_recall,
F1        = rf_f1)
# ROC curve
rf_prob <- predict(rf_model, test2019, type = "prob")[, "Yes"]
pred <- prediction(rf_prob, test2019$Wildfire)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve (SMOTE)")
abline(0, 1, lty = 2, col = "gray")
# Variable importance
importance(rf_model)
varImpPlot(rf_model, main = "Random Forest Feature Importance (SMOTE)")
imp <- importance(rf_model)
imp_df <- data.frame(
Feature    = rownames(imp),
Importance = imp[, "MeanDecreaseGini"]
)
imp_df %>%
ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +
geom_col() +
coord_flip() +
labs(
title = "Feature Importance for Wildfire Prediction (RF + SMOTE)",
x = "Feature",
y = "Mean Decrease in Gini"
) +
theme_minimal(base_size = 13)
# data
path = 'Fire_Incidence_Data 2018-2019.csv'
data = read.csv(path)
str(data)
data$Wildfire = factor(
data$Wildfire,
levels = c('No', 'Yes'))
train_2018 = subset(data, Year == 2018, select = -Year)
test_2019 = subset(data, Year == 2019, select= -Year)
tree_model = rpart(Wildfire ~ .,
data=train_2018,
method = "class",
control = rpart.control(
minsplit = 50,
cp= 0.001))
printcp(tree_model)
rpart.plot(tree_model)
test_2019$tree_pred_class = predict(
tree_model,
newdata = test_2019,
type = "class")
tree_prob = predict(
tree_model,
newdata = test_2019,
type = "prob"
)
test_2019$tree_prob_yes = tree_prob[, "Yes"]
cm_tree <- table(Predicted = test_2019$tree_pred_class,
Actual= test_2019$Wildfire)
cm_tree
TP = cm_tree["Yes", "Yes"]
FP = cm_tree["Yes", "No"]
FN = cm_tree["No",  "Yes"]
TN = cm_tree["No",  "No"]
accuracy  = (TP + TN) / sum(cm_tree)
precision = TP / (TP + FP)
recall= TP / (TP + FN)
f1 <- 2 * precision * recall / (precision + recall)
View(tree_prob)
X = train_2018[, setdiff(names(train_2018), c("Wildfire", "YEAR"))]
y = train_2018$Wildfire
smote_out = SMOTE(X, y, K = 5)
train_smote = smote_out$data
colnames(train_smote)[ncol(train_smote)] = "Wildfire"
train_smote$Wildfire = factor(train_smote$Wildfire, levels = c("No", "Yes"))
tree_smote = rpart(Wildfire ~ .,
data = train_smote,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.001))
test_2019$tree_pred_smote = predict(tree_smote,
newdata = test_2019,
type = "class")
cm_smote = table(Predicted = test_2019$tree_pred_smote,
Actual= test_2019$Wildfire)
cm_smote
TP_s = cm_smote["Yes", "Yes"]
FP_s = cm_smote["Yes", "No"]
FN_s = cm_smote["No",  "Yes"]
TN_s = cm_smote["No",  "No"]
accuracy_s  = (TP_s + TN_s) / sum(cm_smote)
precision_s = TP_s / (TP_s + FP_s)
recall_s= TP_s / (TP_s + FN_s)
f1_s <- 2 * precision_s * recall_s / (precision_s + recall_s)
accuracy_s
precision_s
recall_s
f1_s
printcp(tree_smote)
best_cp <- tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]
best_cp
tree_pruned <- prune(tree_smote, cp = best_cp)
printcp(tree_smote)
best_cp <- tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]
best_cp
tree_pruned <- rpart::prune.rpart(tree_smote, cp = best_cp)
rpart.plot(tree_pruned,type = 1,extra = 1,cex = 0.4,faclen = 3)
test_2019$tree_pred_pruned <- predict(
tree_pruned,
newdata = test_2019,
type = "class"
)
cm_pruned <- table(
Predicted = test_2019$tree_pred_pruned,
Actual    = test_2019$Wildfire
)
cm_pruned
TP_p <- cm_pruned["Yes", "Yes"]
FP_p <- cm_pruned["Yes", "No"]
FN_p <- cm_pruned["No",  "Yes"]
TN_p <- cm_pruned["No",  "No"]
accuracy_p  <- (TP_p + TN_p) / sum(cm_pruned)
precision_p <- TP_p / (TP_p + FP_p)
recall_p    <- TP_p / (TP_p + FN_p)
f1_p <- 2 * precision_p * recall_p / (precision_p + recall_p)
accuracy_p
precision_p
recall_p
f1_p
logit_smote_model <- glm(
Wildfire ~ .,
data   = train_smote,
family = binomial
)
summary(logit_smote_model)
test_2019$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019,
type   = "response"
)
eval_cutoff <- function(prob, actual, cutoff) {
pred <- ifelse(prob >= cutoff, "Yes", "No")
pred <- factor(pred, levels = c("No", "Yes"))
cm <- table(Predicted = pred, Actual = actual)
print(cm)
TP_l <- cm["Yes", "Yes"]
FP_l <- cm["Yes", "No"]
FN_l <- cm["No",  "Yes"]
TN_l <- cm["No",  "No"]
accuracy_l  <- (TP_l + TN_l) / sum(cm)
precision_l <- TP_l / (TP_l + FP_l)
recall_l    <- TP_l / (TP_l + FN_l)
f1_l        <- 2 * precision_l * recall_l / (precision_l + recall_l)
out <- c(
cutoff    = cutoff,
accuracy  = accuracy_l,
precision = precision_l,
recall    = recall_l,
f1        = f1_l
)
return(out)
}
cutoffs <- c(0.1, 0.2, 0.3, 0.5)
results_logit_smote <- t(
sapply(cutoffs, function(c) {
eval_cutoff(
prob   = test_2019$prob_yes_smote,
actual = test_2019$Wildfire,
cutoff = c
)
})
)
results_logit_smote <- as.data.frame(results_logit_smote)
results_logit_smote[] <- lapply(results_logit_smote, as.numeric)
knitr::kable(results_logit_smote, digits = 4)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(
response  = y_logit,
predictor = test_2019_logit$prob_yes_smote
)
plot(roc_logit, col = "red", lwd = 2,
main = "ROC Curve – Logistic Regression with SMOTE")
auc(roc_logit)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(test_2019_logit$Wildfire,
test_2019_logit$prob_yes_smote,
levels = c("No","Yes"))
roc_rf <- roc(test_2019$Wildfire, rf_prob, levels = c("No","Yes"))
roc_svm <- roc(test_2019$Wildfire, svm_prob, levels = c("No","Yes"))
roc_knn <- roc(fire_prob_smote$Wildfire, fire_prob_smote$.pred_Yes, levels = c("No","Yes"))
roc_tree <- roc(test_2019$Wildfire,
test_2019$tree_prob_yes,
levels = c("No","Yes"))
tree_pruned_prob <- predict(tree_pruned, test_2019, type="prob")[,"Yes"]
roc_tree_pruned <- roc(test_2019$Wildfire,
tree_pruned_prob,
levels = c("No","Yes"))
plot(roc_logit, col="red", lwd=2,
main="ROC Comparison – All Models")
plot(roc_rf, col="blue", lwd=2, add=TRUE)
plot(roc_svm, col="green", lwd=2, add=TRUE)
plot(roc_knn, col="orange", lwd=2, add=TRUE)
plot(roc_tree, col="purple", lwd=2, add=TRUE)
plot(roc_tree_pruned, col="brown", lwd=2, add=TRUE)
legend("bottomright",
legend=c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN",
"Decision Tree", "Pruned Tree"),
col=c("red","blue","green","orange","purple","brown"),
lwd=2)
ggplot(compare_long, aes(x = Model, y = Score, fill = Metric)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("steelblue", "orange")) +
labs(
title = "Recall and F1 Score Comparison Across Models",
x = "",
y = "Score"
) +
theme_minimal(base_size = 13)
recall_logit <- results_logit_smote$recall[results_logit_smote$cutoff == 0.5]
f1_logit     <- results_logit_smote$f1[results_logit_smote$cutoff == 0.5]
recall_rf  <- rf_recall
f1_rf      <- rf_f1
recall_svm <- svm_recall
f1_svm     <- svm_f1
recall_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "recall") %>% pull(.estimate)
f1_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "f_meas") %>% pull(.estimate)
recall_tree <- recall
f1_tree     <- f1
recall_pruned <- recall_p
f1_pruned     <- f1_p
# ---- Combine into a single dataframe ----
model_compare <- data.frame(
Model   = c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN", "Decision Tree", "Pruned Tree"),
Recall  = c(recall_logit, recall_rf, recall_svm, recall_knn, recall_tree, recall_pruned),
F1      = c(f1_logit, f1_rf, f1_svm, f1_knn, f1_tree, f1_pruned)
)
knitr::kable(model_compare, digits = 4)
library(tidyverse)
# Long format for ggplot
compare_long <- model_compare %>%
pivot_longer(cols = c("Recall", "F1"),
names_to = "Metric",
values_to = "Score")
ggplot(compare_long, aes(x = Model, y = Score, fill = Metric)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("steelblue", "orange")) +
labs(
title = "Recall and F1 Score Comparison Across Models",
x = "",
y = "Score"
) +
theme_minimal(base_size = 13)
recall_logit <- results_logit_smote$recall[results_logit_smote$cutoff == 0.5]
f1_logit     <- results_logit_smote$f1[results_logit_smote$cutoff == 0.5]
recall_rf  <- rf_recall
f1_rf      <- rf_f1
recall_svm <- svm_recall
f1_svm     <- svm_f1
recall_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "recall") %>% pull(.estimate)
f1_knn <- knn_metrics(
fire_predictions_smote,
truth = Wildfire,
estimate = .pred_class,
event_level = "second"
) %>% filter(.metric == "f_meas") %>% pull(.estimate)
recall_tree <- recall
f1_tree     <- f1
recall_pruned <- recall_p
f1_pruned     <- f1_p
# ---- Combine into a single dataframe ----
model_compare <- data.frame(
Model   = c("Logistic", "Random Forest", "SVM", "KNN", "Decision Tree", "Pruned Tree"),
Recall  = c(recall_logit, recall_rf, recall_svm, recall_knn, recall_tree, recall_pruned),
F1      = c(f1_logit, f1_rf, f1_svm, f1_knn, f1_tree, f1_pruned)
)
knitr::kable(model_compare, digits = 4)
library(tidyverse)
# Long format for ggplot
compare_long <- model_compare %>%
pivot_longer(cols = c("Recall", "F1"),
names_to = "Metric",
values_to = "Score")
ggplot(compare_long, aes(x = Model, y = Score, fill = Metric)) +
geom_col(position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("steelblue", "orange")) +
labs(
title = "Recall and F1 Score Comparison Across Models",
x = "",
y = "Score"
) +
theme_minimal(base_size = 13)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(test_2019_logit$Wildfire,
test_2019_logit$prob_yes_smote,
levels = c("No","Yes"))
roc_rf <- roc(test_2019$Wildfire, rf_prob, levels = c("No","Yes"))
roc_svm <- roc(test_2019$Wildfire, svm_prob, levels = c("No","Yes"))
roc_knn <- roc(fire_prob_smote$Wildfire, fire_prob_smote$.pred_Yes, levels = c("No","Yes"))
roc_tree <- roc(test_2019$Wildfire,
test_2019$tree_prob_yes,
levels = c("No","Yes"))
tree_pruned_prob <- predict(tree_pruned, test_2019, type="prob")[,"Yes"]
roc_tree_pruned <- roc(test_2019$Wildfire,
tree_pruned_prob,
levels = c("No","Yes"))
plot(roc_logit, col="red", lwd=2,
main="ROC Comparison – All Models")
plot(roc_rf, col="blue", lwd=2, add=TRUE)
plot(roc_svm, col="green", lwd=2, add=TRUE)
plot(roc_knn, col="orange", lwd=2, add=TRUE)
plot(roc_tree, col="purple", lwd=2, add=TRUE)
plot(roc_tree_pruned, col="brown", lwd=2, add=TRUE)
legend("bottomright",
legend=c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN",
"Decision Tree", "Pruned Tree"),
col=c("red","blue","green","orange","purple","brown"),
lwd=2)
path <- "Fire_Incidence_Data 2018-2019.csv"
data_full <- read.csv(path)
data_full$Wildfire <- factor(data_full$Wildfire,
levels = c("No", "Yes"))
test_2019_logit <- subset(data_full, Year == 2019, select = -Year)
test_2019_logit$prob_yes_smote <- predict(
logit_smote_model,
newdata = test_2019_logit,
type   = "response"
)
y_logit <- ifelse(test_2019_logit$Wildfire == "Yes", 1, 0)
roc_logit <- roc(test_2019_logit$Wildfire,
test_2019_logit$prob_yes_smote,
levels = c("No","Yes"))
roc_rf <- roc(test_2019$Wildfire, rf_prob, levels = c("No","Yes"))
roc_svm <- roc(test_2019$Wildfire, svm_prob, levels = c("No","Yes"))
roc_knn <- roc(fire_prob_smote$Wildfire, fire_prob_smote$.pred_Yes, levels = c("No","Yes"))
roc_tree <- roc(test_2019$Wildfire,
test_2019$tree_prob_yes,
levels = c("No","Yes"))
tree_pruned_prob <- predict(tree_pruned, test_2019, type="prob")[,"Yes"]
roc_tree_pruned <- roc(test_2019$Wildfire,
tree_pruned_prob,
levels = c("No","Yes"))
plot(roc_logit, col="red", lwd=2,
main="ROC Comparison – All Models")
plot(roc_rf, col="blue", lwd=2, add=TRUE)
plot(roc_svm, col="green", lwd=2, add=TRUE)
plot(roc_knn, col="orange", lwd=2, add=TRUE)
plot(roc_tree, col="purple", lwd=2, add=TRUE)
plot(roc_tree_pruned, col="brown", lwd=2, add=TRUE)
legend("bottomright",
legend=c("Logistic (SMOTE)", "Random Forest", "SVM", "KNN",
"Decision Tree", "Pruned Tree"),
col=c("red","blue","green","orange","purple","brown"),
lwd=2)
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)
knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
mutate(Wildfire=trimws(Wildfire))%>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
data = read_csv("Fire_Incidence_Data 2018-2019.csv")
ratio_df <- data %>%
count(Wildfire) %>%
mutate(percent = n / sum(n) * 100)
ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
geom_col(width = 0.6) +
scale_fill_manual(values = c("steelblue", "tomato")) +
labs(
title = "Wildfire Class Distribution (%)",
x = "Wildfire",
y = "Percentage"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "none")
