---
title: "1"
output: html_document
Name : 'Jeongwon Yoo'
date: "2025-11-16"
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(tidymodels)
library(rpart)
library(rpart.plot)
library(smotefamily)
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# data
path = '/Users/jeongwonyoo/Documents/GitHub/introds-project2/Fire_Incidence_Data 2018-2019.csv'
data = read.csv(path)
```

# Data Explore

```{r}
str(data)
```

```{r}
data$Wildfire = factor(
  data$Wildfire,
  levels = c('No', 'Yes'))

train_2018 = subset(data, Year == 2018, select = -Year)
test_2019 = subset(data, Year == 2019, select= -Year)

```

# logistic Regression

```{r}
logit_model = glm(Wildfire ~.,
                  data = train_2018,
                  family = binomial)
summary(logit_model)
```

cutoff 0.5

```{r}
test_2019$prob_yes = predict(logit_model,
                              newdata = test_2019,
                              type = "response")

test_2019$pred = ifelse(test_2019$prob_yes >= 0.5, "Yes", "No")
test_2019$pred = factor(test_2019$pred, levels = c("No","Yes"))
 
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.1

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.1, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.2

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.2, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.3

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.3, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

I evaluated the logistic regression model, and the outcomes confirmed that the target variable is highly imbalanced. With a cutoff of 0.5, the model predicted all cases as “No.” Even after lowering the cutoff to 0.1, the model only recovered a small number of “Yes” cases, and both precision and recall remained very low. This indicates that logistic regression is not suitable for this dataset.

# Decision Tree

```{r}
tree_model = rpart(Wildfire ~ .,
                    data=train_2018,
                    method = "class",
                    control = rpart.control(
                      minsplit = 50,
                      cp= 0.001))
printcp(tree_model)

rpart.plot(tree_model)
test_2019$tree_pred_class = predict(
  tree_model,
  newdata = test_2019,
  type = "class")

tree_prob = predict(
  tree_model,
  newdata = test_2019,
  type = "prob"
)

test_2019$tree_prob_yes = tree_prob[, "Yes"]

cm_tree <- table(Predicted = test_2019$tree_pred_class,
                 Actual= test_2019$Wildfire)
cm_tree

TP = cm_tree["Yes", "Yes"]
FP = cm_tree["Yes", "No"]
FN = cm_tree["No",  "Yes"]
TN = cm_tree["No",  "No"]

accuracy  = (TP + TN) / sum(cm_tree)
precision = TP / (TP + FP)
recall= TP / (TP + FN)
View(tree_prob)
```

SMOTE the data
```{r}
X = train_2018[, setdiff(names(train_2018), c("Wildfire", "YEAR"))]
y = train_2018$Wildfire
smote_out = SMOTE(X, y, K = 5)
train_smote = smote_out$data
colnames(train_smote)[ncol(train_smote)] = "Wildfire"

train_smote$Wildfire = factor(train_smote$Wildfire, levels = c("No", "Yes"))

tree_smote = rpart(Wildfire ~ .,
                   data = train_smote,
                   method = "class",
                   control = rpart.control(minsplit = 50, cp = 0.001))

test_2019$tree_pred_smote = predict(tree_smote,
                                    newdata = test_2019,
                                    type = "class")

cm_smote = table(Predicted = test_2019$tree_pred_smote,
                 Actual= test_2019$Wildfire)
cm_smote

TP = cm_smote["Yes", "Yes"]
FP = cm_smote["Yes", "No"]
FN = cm_smote["No",  "Yes"]
TN = cm_smote["No",  "No"]

accuracy  = (TP + TN) / sum(cm_smote)
precision = TP / (TP + FP)
recall= TP / (TP + FN)

```

smote data has high risk of noise
Pruning tree
```{r}
printcp(tree_smote)

best_cp <- tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]

best_cp

tree_pruned <- prune(tree_smote, cp = best_cp)

rpart.plot(tree_pruned,type = 1,extra = 1,cex = 0.4,faclen = 3)

test_2019$tree_pred_pruned <- predict(
  tree_pruned,
  newdata = test_2019,
  type = "class"
)

cm_pruned <- table(
  Predicted = test_2019$tree_pred_pruned,
  Actual    = test_2019$Wildfire
)
cm_pruned

TP <- cm_pruned["Yes", "Yes"]
FP <- cm_pruned["Yes", "No"]
FN <- cm_pruned["No",  "Yes"]
TN <- cm_pruned["No",  "No"]

accuracy_pruned  <- (TP + TN) / sum(cm_pruned)
precision_pruned <- TP / (TP + FP)
recall_pruned    <- TP / (TP + FN)

accuracy_pruned
precision_pruned
recall_pruned
```

The pruned decision tree produced the same evaluation metrics as the original SMOTE-trained tree. After pruning, the model has an accuracy of about 77 percent, precision around 5 percent, and recall around 29.5 percent for the “Yes” class. This means pruning simplified the tree structure but did not meaningfully change how the model classifies wildfire events.
The recall is still much higher than the original (non-SMOTE) model, but the false positive count remains large, which keeps the precision low. Overall, SMOTE improved the model’s ability to detect rare “Yes” cases, and pruning helped reduce model complexity without affecting performance.