---
title: "1"
output: html_document
Name : 'Jeongwon Yoo'
date: "2025-11-16"
---

```{r setup, include=FALSE}
library(readr)
library(dplyr)
library(tidymodels)
library(rpart)
library(rpart.plot)
library(smotefamily)
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# data
path = '/Users/jeongwonyoo/Documents/GitHub/introds-project2/Fire_Incidence_Data 2018-2019.csv'
data = read.csv(path)
```

# Data Explore

```{r}
str(data)
```

```{r}
data$Wildfire = factor(
  data$Wildfire,
  levels = c('No', 'Yes'))

train_2018 = subset(data, Year == 2018, select = -Year)
test_2019 = subset(data, Year == 2019, select= -Year)

```

# logistic Regression

```{r}
logit_model = glm(Wildfire ~.,
                  data = train_2018,
                  family = binomial)
summary(logit_model)
```

cutoff 0.5

```{r}
test_2019$prob_yes = predict(logit_model,
                              newdata = test_2019,
                              type = "response")

test_2019$pred = ifelse(test_2019$prob_yes >= 0.5, "Yes", "No")
test_2019$pred = factor(test_2019$pred, levels = c("No","Yes"))
 
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.1

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.1, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.2

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.2, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

cutoff 0.3

```{r}
test_2019$pred = ifelse(test_2019$prob_yes >= 0.3, "Yes", "No")
cm = table(Predicted = test_2019$pred, Actual= test_2019$Wildfire)
cm
```

I evaluated the logistic regression model, and the outcomes confirmed that the target variable is highly imbalanced. With a cutoff of 0.5, the model predicted all cases as “No.” Even after lowering the cutoff to 0.1, the model only recovered a small number of “Yes” cases, and both precision and recall remained very low. This indicates that logistic regression is not suitable for this dataset.

# Decision Tree

```{r}
tree_model = rpart(Wildfire ~ .,
                    data=train_2018,
                    method = "class",
                    control = rpart.control(
                      minsplit = 50,
                      cp= 0.001))
printcp(tree_model)

rpart.plot(tree_model)
test_2019$tree_pred_class = predict(
  tree_model,
  newdata = test_2019,
  type = "class")

tree_prob = predict(
  tree_model,
  newdata = test_2019,
  type = "prob"
)

test_2019$tree_prob_yes = tree_prob[, "Yes"]

cm_tree <- table(Predicted = test_2019$tree_pred_class,
                 Actual= test_2019$Wildfire)
cm_tree

TP = cm_tree["Yes", "Yes"]
FP = cm_tree["Yes", "No"]
FN = cm_tree["No",  "Yes"]
TN = cm_tree["No",  "No"]

accuracy  = (TP + TN) / sum(cm_tree)
precision = TP / (TP + FP)
recall= TP / (TP + FN)
View(tree_prob)
```

SMOTE the data
```{r}
X = train_2018[, setdiff(names(train_2018), c("Wildfire", "YEAR"))]
y = train_2018$Wildfire
smote_out = SMOTE(X, y, K = 5)
train_smote = smote_out$data
colnames(train_smote)[ncol(train_smote)] = "Wildfire"

train_smote$Wildfire = factor(train_smote$Wildfire, levels = c("No", "Yes"))

tree_smote = rpart(Wildfire ~ .,
                   data = train_smote,
                   method = "class",
                   control = rpart.control(minsplit = 50, cp = 0.001))

test_2019$tree_pred_smote = predict(tree_smote,
                                    newdata = test_2019,
                                    type = "class")

cm_smote = table(Predicted = test_2019$tree_pred_smote,
                 Actual= test_2019$Wildfire)
cm_smote

TP = cm_smote["Yes", "Yes"]
FP = cm_smote["Yes", "No"]
FN = cm_smote["No",  "Yes"]
TN = cm_smote["No",  "No"]

accuracy  = (TP + TN) / sum(cm_smote)
precision = TP / (TP + FP)
recall= TP / (TP + FN)

```
