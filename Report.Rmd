---
title: "Forecasting Wildfire Events from Climate Indicators: A Comparative ML Study"
author:
  Jeongwon Yoo
  Nithin Ravindra Reddy
  Simbanegavi Simbarashe 
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

library(caret)
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)

knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)


```

# CHAPTER 1: Introduction

## 1.0 Overview

Wildfires have become an increasingly severe environmental and societal challenge, driven by changing climate patterns, prolonged droughts, and expanding human activity. In recent years, wildfire incidents have increased in both frequency and severity, resulting in substantial ecological damage, economic loss, and risks to human life. Accurate prediction of wildfire occurrence is therefore a critical component of disaster preparedness and risk mitigation.

This study focuses on forecasting wildfire events using climate and spatial indicators derived from U.S. wildfire incidence data from 2018 to 2019. Multiple machine learning classification models are applied to examine how climate variables such as temperature, humidity, radiation, and precipitation relate to wildfire occurrence. A key challenge addressed in this study is the extreme class imbalance between wildfire and non-wildfire events, which complicates reliable model evaluation and motivates the use of recall-focused performance metrics.

This study also emphasizes interpretability by examining feature importance and detection trade-offs across models.

## 1.1 Objectives
The primary objectives of this study are:

1) To develop and train multiple machine learning classification models, including Logistic Regression, K-Nearest Neighbors, Support Vector Machines, Decision Trees, and Random Forests, to predict wildfire occurrence using climate and spatial features.

2) To evaluate and compare model performance using metrics such as accuracy, precision, recall, F1-score, Kappa, and ROC-AUC, with particular emphasis on minority class detection.

3) To identify influential climate variables associated with wildfire occurrence through model interpretation techniques such as feature importance analysis.

## 1.2 Research Questions
This study is guided by the following research questions:

1) Can machine learning models predict wildfire occurrence using climate and spatial indicators when trained on 2018 data and evaluated on 2019 data?

2) How do different machine learning algorithms compare in their ability to detect wildfire events under highly imbalanced data conditions?

3) Which climate-related variables most strongly influence the likelihood of wildfire occurrence?

This study is primarily prediction-focused, aiming to assess how well machine learning models can identify wildfire occurrence under realistic forecasting conditions rather than to infer causal relationships.

## 1.3 Significance of the Study

This research contributes to the literature on climate-driven wildfire prediction by providing a comparative evaluation of multiple machine learning approaches using temporally split real-world data. Rather than relying solely on overall accuracy, this study emphasizes evaluation metrics that are critical for detecting rare wildfire events.

From a practical perspective, the findings offer insights into how climate indicators can support early warning systems and wildfire risk assessment. Identifying influential predictors and understanding model behavior can assist policymakers, environmental agencies, and emergency response teams in developing more targeted prevention and monitoring strategies. The study also demonstrates a reproducible machine learning workflow in R that follows best practices for handling imbalanced environmental datasets.

# CHAPTER 2: EDA

Wildfire Yes/No Ratio

```{r}
data = read_csv("Fire_Incidence_Data 2018-2019.csv")

ratio_df <- data %>%
  count(Wildfire) %>%
  mutate(percent = n / sum(n) * 100)

ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
  geom_col(width = 0.6) +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Wildfire Class Distribution (%)",
    x = "Wildfire",
    y = "Percentage"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```
The dataset is highly imbalanced, with far fewer wildfire events than non-wildfire cases. This motivates imbalance handling and recall-focused evaluation in later sections.

Correlation Matrix
```{r}
num_vars <- data%>%dplyr::select(where(is.numeric))

corr_mat <- cor(num_vars, method = "pearson")

options(repr.plot.width = 14, repr.plot.height = 12)

corrplot(
  corr_mat,
  method = "number",
  type = "upper",
  tl.col = "black",
  number.cex = 0.7,
  tl.cex = 0.8
)
```
Strong correlation clusters are observed among temperature (tmmn–tmmx), fuel moisture (fm100–fm1000), and atmospheric demand variables (etr, pet, vpd), indicating partially overlapping information among predictors.

# CHAPTER 3: Methodology

## 3.1 Preprocessing pipeline

```{r}
model = data %>%
  mutate(Wildfire = trimws(Wildfire),
         Wildfire = factor(Wildfire, levels = c('No','Yes')))%>%
  tidyr::drop_na()

table(model$Wildfire)
table(model$Year)
```

## 3.2 Train & Test split(SMOTE)

```{r}
train_2018 = model %>% filter(Year==2018) %>% select(-Year)
test_2019 = model %>% filter(Year==2019) %>% select(-Year)

smote = function(df,target='Wildfire',K=5,seed=2){
  df = as.data.frame(df)
  
  y = df[[target]]
  X = df[,setdiff(names(df), target), drop = FALSE]
  
   y_num = ifelse(y =='Yes',1,0)
   
   set.seed(seed)
   sm = smotefamily::SMOTE(X, y_num, K=K)
   
   out = sm$data
   colnames(out)[ncol(out)] = 'y_num'
   
   out[[target]] = factor(
     ifelse(out$y_num == 1, 'Yes','No'),
     levels = c('No','Yes')
   )
   
   out$y_num = NULL
   
   out = out[,c(setdiff(names(out), target), target)]
   
   return(out)
}

train_smote = smote(train_2018, target='Wildfire', K=5, seed=2)
test_smote = smote(test_2019, target='Wildfire',K=5,seed=2)

table(train_smote$Wildfire)
table(test_smote$Wildfire)
```

After applying SMOTE, the class counts become balanced, which prevents evaluation metrics from being dominated by the majority class. This allows model performance to be compared more directly in terms of wildfire detection ability and error trade-offs.

SMOTE was applied consistently to both training and test sets to allow fair comparison across models under balanced evaluation conditions.


## 3.3 Modeling strategy

### 3.3.1 KNN

```{r}
fire_recipe_knn = recipe(Wildfire~., data = train_smote) %>% step_normalize(all_numeric_predictors())
```

1. Define the KNN Classification Model
```{r}
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")

knn_model
```

2. Create the Workflow
```{r}
knn_wf <- workflow() %>%
add_recipe(fire_recipe_knn) %>%
add_model(knn_model)

knn_wf
```

3. Train the Model on 2018 Data
```{r}
knn_fit <- fit(knn_wf, data = train_smote)
```

4. Predict Wildfire Occurrence for 2019
```{r}
knn_predictions <- predict(knn_fit, test_smote) %>%
bind_cols(test_smote)

head(knn_predictions)
```

5. Confusion Matrix (Prediction Performance)
```{r}
conf_mat(
knn_predictions,
truth = Wildfire,
estimate = .pred_class
)
```

KNN shows moderate wildfire detection after SMOTE, but recall remains limited, indicating many missed wildfire events.

6. Classification Metrics (Accuracy, Recall, Precision, F1)

```{r metrics}
knn_metrics = yardstick::metric_set(
  yardstick::accuracy,
  yardstick::precision,
  yardstick::recall,
  yardstick::f_meas
)

knn_metrics(
  knn_predictions,
  truth = Wildfire,
  estimate = .pred_class,
  event_level = 'second'
)
```

While the model achieves relatively high precision, recall remains moderate, indicating that a substantial number of wildfire events are still misclassified as non-wildfire. This suggests that KNN tends to be conservative in issuing wildfire predictions, favoring precision over sensitivity, which limits its usefulness in high-risk detection scenarios.

7. ROC-AUC 

```{r knn-prob}
# Get predicted probabilities for both classes
knn_prob <- predict(knn_fit, test_smote, type = "prob") %>%
  bind_cols(test_smote)
```
```{r roc-plot, fig.width=7, fig.height=6}
# Create ROC object
roc_obj <- roc_curve(
  knn_prob,
  truth = Wildfire,
  .pred_Yes,
  event_level = "second"
)

# Plot ROC curve using ggplot
autoplot(roc_obj) +
  ggtitle("ROC Curve for KNN Wildfire Prediction (2019)") +
  theme_minimal()

```

The ROC curve suggests modest discriminative ability, consistent with the confusion matrix results.

### 3.3.2 SVM

```{r}
svm_model <- svm(
  Wildfire ~ .,
  data = train_smote,
  kernel = "radial",
  probability = TRUE,
  scale = TRUE
)

# Predictions
svm_pred <- predict(svm_model, test_smote)
svm_cm <- table(Predicted = svm_pred, Actual = test_smote$Wildfire)
svm_cm
```

```{r}
# Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)

# Metrics
TP_svm <- svm_cm["Yes", "Yes"]
FN_svm <- svm_cm["No",  "Yes"]
FP_svm <- svm_cm["Yes", "No"]

svm_precision <- TP_svm / (TP_svm + FP_svm)
svm_recall    <- TP_svm / (TP_svm + FN_svm)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
 
c(
  accuracy  = svm_accuracy,
  precision = svm_precision,
  recall    = svm_recall,
  F1        = svm_f1
)
```

```{r}
# Probability prediction
svm_prob <- attr(
  predict(svm_model, test_smote, probability = TRUE),
  "probabilities"
)[, "Yes"]

# ROCR performance objects
pred_svm <- prediction(svm_prob, test_smote$Wildfire)
perf_svm <- performance(pred_svm, "tpr", "fpr")

# Plot ROC curve
plot(perf_svm, col = "red", lwd = 2, main = "SVM ROC Curve")
abline(0, 1, lty = 2, col = "gray")

# Compute AUC
svm_auc <- performance(pred_svm, "auc")@y.values[[1]]
svm_auc
```

SVM improves recall and F1 slightly compared to KNN, but many wildfire cases are still missed. The ROC curve indicates only marginal ranking improvement, so the gain over KNN is limited.

### 3.3.3 Random Forest

```{r}
# Random Forest on SMOTE balanced data

rf_model <- randomForest(
  Wildfire ~ .,
  data = train_smote,
  ntree = 500,
  importance = TRUE
)

# Predictions
rf_pred <- predict(rf_model, test_smote)
rf_cm <- table(Predicted = rf_pred, Actual = test_smote$Wildfire)
rf_cm
```


```{r}
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)

TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]

rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)

c(accuracy = rf_accuracy,
  precision = rf_precision,
  recall    = rf_recall,
  F1        = rf_f1)

```

```{r}
# ROC curve
rf_prob <- predict(rf_model, newdata=test_smote, type = "prob")[,"Yes"]

pred <- prediction(rf_prob, test_smote$Wildfire)
perf <- performance(pred, "tpr", "fpr")

# AUC
auc_perf <- performance(pred, "auc")
rf_auc <- auc_perf@y.values[[1]]
rf_auc


plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve")
abline(0, 1, lty = 2, col = "gray")
```

Random Forest achieves strong overall performance and the best ROC ranking ability, but recall remains only moderate under the chosen threshold. This limits its suitability when minimizing missed wildfire events is the primary objective.

### 3.3.4 Decision Tree

```{r}
tree_smote = rpart(Wildfire ~ .,
                   data = train_smote,
                   method = "class",
                   control = rpart.control(minsplit = 50, cp = 0.001))

test_smote$tree_pred_smote = predict(tree_smote,
                                    newdata = test_smote,
                                    type = "class")

cm_smote = table(Predicted = test_smote$tree_pred_smote,
                 Actual= test_smote$Wildfire)
cm_smote
```

```{r}
TP_s = cm_smote["Yes", "Yes"]
FP_s = cm_smote["Yes", "No"]
FN_s = cm_smote["No",  "Yes"]
TN_s = cm_smote["No",  "No"]

accuracy_s  = (TP_s + TN_s) / sum(cm_smote)
precision_s = TP_s / (TP_s + FP_s)
recall_s= TP_s / (TP_s + FN_s)
f1_s <- 2 * precision_s * recall_s / (precision_s + recall_s)

accuracy_s
precision_s
recall_s
f1_s

```
Because SMOTE can introduce synthetic noise, pruning is used to reduce overfitting and improve generalization.

```{r}
printcp(tree_smote)

best_cp = tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]

best_cp

tree_pruned = rpart::prune(tree_smote, cp = best_cp)

rpart.plot(tree_pruned,type = 1,extra = 1,cex = 0.4,faclen = 3)

test_smote$tree_pred_pruned <- predict(
  tree_pruned,
  newdata = test_smote,
  type = "class"
)

cm_pruned <- table(
  Predicted = test_smote$tree_pred_pruned,
  Actual    = test_smote$Wildfire
)
cm_pruned

TP_p <- cm_pruned["Yes", "Yes"]
FP_p <- cm_pruned["Yes", "No"]
FN_p <- cm_pruned["No",  "Yes"]
TN_p <- cm_pruned["No",  "No"]

accuracy_p  <- (TP_p + TN_p) / sum(cm_pruned)
precision_p <- TP_p / (TP_p + FP_p)
recall_p    <- TP_p / (TP_p + FN_p)
f1_p <- 2 * precision_p * recall_p / (precision_p + recall_p)

accuracy_p
precision_p
recall_p
f1_p

tree_prob <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]

tree_prob <- data.frame(
  .pred_Yes = tree_prob,
  model = "Decision Tree"
)
```

After pruning, the decision tree shows more balanced performance, with reduced false positives and improved generalization compared to the unpruned model. Although recall decreases slightly, the overall trade-off between precision and recall becomes more stable, indicating that pruning effectively controls overfitting while preserving the model’s ability to detect wildfire events.

```{r}
# Create ROC object
tree_prob_yes <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]
tree_prob_yes <- as.numeric(tree_prob_yes)

tree_df <- data.frame(
  Wildfire  = test_smote$Wildfire,
  .pred_Yes = tree_prob_yes
)

names(tree_df)

roc_dt <- roc_curve(tree_df,
                    truth = Wildfire,
                    .pred_Yes,
                    event_level = 'second')

# Plot ROC curve using ggplot
autoplot(roc_dt) +
  ggtitle("ROC Curve for Pruned Decision Tree (2019)") +
  theme_minimal()
```
The ROC curve for the pruned decision tree indicates moderate to strong discriminative ability. While its AUC is lower than that of the Random Forest, the curve demonstrates consistent separation across thresholds. This supports the use of the decision tree as a final model, balancing interpretability with acceptable predictive performance in wildfire detection.

### 3.3.5 Logistic Regression

logistic regression with SMOTE

```{r}
logit_smote_model <- glm(
  Wildfire ~ .,
  data   = train_smote,
  family = binomial
)

summary(logit_smote_model)
```
Several climate predictors are statistically significant, but the model’s linear form limits its ability to capture non-linear wildfire patterns. As a result, wildfire detection performance remains weaker than tree-based models.

```{r}
test_smote$prob_yes_smote <- predict(
  logit_smote_model,
  newdata = test_smote,
  type   = "response"
)

eval_cutoff <- function(prob, actual, cutoff) {
  pred <- ifelse(prob >= cutoff, "Yes", "No")
  pred <- factor(pred, levels = c("No", "Yes"))
  
  cm <- table(Predicted = pred, Actual = actual)
  print(cm)
  
  TP_l <- cm["Yes", "Yes"]
  FP_l <- cm["Yes", "No"]
  FN_l <- cm["No",  "Yes"]
  TN_l <- cm["No",  "No"]
  
  accuracy_l  <- (TP_l + TN_l) / sum(cm)
  precision_l <- TP_l / (TP_l + FP_l)
  recall_l    <- TP_l / (TP_l + FN_l)
  f1_l        <- 2 * precision_l * recall_l / (precision_l + recall_l)
  
  out <- c(
    cutoff    = cutoff,
    accuracy  = accuracy_l,
    precision = precision_l,
    recall    = recall_l,
    f1        = f1_l
  )
  
  return(out)
}

cutoffs <- c(0.1, 0.2, 0.3, 0.5)

results_logit_smote <- t(
  sapply(cutoffs, function(c) {
    eval_cutoff(
      prob   = test_smote$prob_yes_smote,
      actual = test_smote$Wildfire,
      cutoff = c
    )
  })
)

results_logit_smote <- as.data.frame(results_logit_smote)
results_logit_smote[] <- lapply(results_logit_smote, as.numeric)

knitr::kable(results_logit_smote, digits = 4)

logit_prob <- predict(logit_smote_model, test_smote, type = "response")

logit_prob <- data.frame(
  .pred_Yes = logit_prob,
  model = "Logistic Regression"
)

```
Lower cutoffs improve recall at the expense of precision, with the 0.1 threshold showing signs of overfitting due to excessive false positives. Higher cutoffs yield more stable performance, underscoring the need for careful cutoff selection in wildfire detection. Therefore, a cutoff of 0.5 was chosen as the final cutoff to ensure balanced and stable performance.

```{r}
# Create ROC object

logit_prob_yes <- predict(
  logit_smote_model,
  newdata = test_smote,
  type = "response"
)

logit_df = data.frame(Wildfire = test_smote$Wildfire,
                      .pred_Yes = logit_prob_yes)

roc_logit <- roc_curve(
  logit_df,
  truth = Wildfire,
  .pred_Yes,
  event_level = "second"
)

# Plot ROC curve using ggplot
autoplot(roc_logit) +
  ggtitle("ROC Curve for Logistic Regression (2019)") +
  theme_minimal()
```
The ROC curve for logistic regression indicates limited discriminative power, reflecting the constraints of a linear model in capturing complex wildfire patterns.

# CHAPTER 4: Conclusion & Future Works

## 4.1 Summary of Findings

This study compared multiple machine learning models for wildfire prediction under severe class imbalance. While several models showed reasonable performance, their strengths differed depending on the evaluation metric. Models with strong ranking ability did not always achieve high recall, highlighting the importance of metric selection. Overall, the pruned decision tree provided the most balanced trade-off between wildfire detection performance, generalization, and interpretability.

## 4.2 Best Performing Model and Justification

In selecting the best-performing model, recall was prioritized as the primary evaluation criterion, as failing to detect wildfire events carries greater risk than issuing false alarms. F1-score was considered alongside recall to account for the trade-off between sensitivity and precision, while ROC curves were used as a supporting metric to assess overall ranking ability across cutoffs. Based on this evaluation framework, models were compared to identify the approach that best balances wildfire detection performance, generalization, and interpretability.

### 4.2.1. Comparing Recall & F1 Score
```{r}
logit_prob <- predict(logit_smote_model, newdata = test_smote, type = "response")
logit_prob <- as.numeric(logit_prob)

knn_pred_class = predict(knn_fit, test_smote)$.pred_class
svm_pred_class = predict(svm_model, test_smote)
rf_pred_class = predict(rf_model, newdata=test_smote, type='class')
dt_pred_class = predict(tree_pruned, newdata = test_smote, type='class')
logit_pred_class = factor(ifelse(logit_prob >= 0.5, 'Yes','No'), levels = c('No','Yes'))

fix_levels = function(x) factor(x, levels = c('No','Yes'))
truth_fix = fix_levels(test_smote$Wildfire)

knn_pred_class = fix_levels(knn_pred_class)
svm_pred_class = fix_levels(svm_pred_class)
rf_pred_class = fix_levels(rf_pred_class)
dt_pred_class = fix_levels(dt_pred_class)
logit_pred_class = fix_levels(logit_pred_class)

metric_c = yardstick::metric_set(yardstick::recall,yardstick::f_meas)
metrics_df = bind_rows(
  tibble(model='KNN', truth = truth_fix, estimate = knn_pred_class),
  tibble(model='SVM', truth = truth_fix, estimate = svm_pred_class),
  tibble(model='Random Forest', truth = truth_fix, estimate = rf_pred_class),
  tibble(model='Decision Tree', truth = truth_fix, estimate = dt_pred_class),
  tibble(model='Logistic Regression', truth = truth_fix, estimate = logit_pred_class)
)%>%
  group_by(model)%>%
  metric_c(truth=truth, estimate = estimate, event_level = 'second')%>%
  ungroup()%>%
  select(model, .metric,.estimate)

ggplot(metrics_df, aes(x=model, y=.estimate, fill=.metric)) + geom_col(position = position_dodge(width=0.8), width =0.7) + coord_flip()+ labs(
  title = 'Comparing Recall and F1 Score Across Models(SMOTE-balanced)',
  x='Models',
  y='Score',
  fill='Metric'
)+
  theme_minimal(base_size=14) +
  theme(axis.text.x = element_text(angle=25, hjust=1))
```
Across models, the pruned decision tree provides the best balance between recall and F1-score while remaining interpretable. Although Random Forest performs strongly overall, its recall is not consistently higher under the selected threshold, supporting the decision tree as the final model.

### 4.2.2 Comparing AUC-ROC

ROC analysis is used here as a supporting comparison metric rather than a primary selection criterion.

```{r}
rm(roc_df)
gc()

# KNN
knn_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = predict(knn_fit, test_smote, type = "prob")$.pred_Yes,
  model = "KNN"
)

# SVM
svm_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(svm_prob),
  model = "SVM"
)

# Random Forest
rf_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(rf_prob),
  model = "Random Forest"
)

# Decision Tree
tree_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(tree_prob$.pred_Yes),
  model = "Decision Tree"
)

# Logistic
logit_vec <- if (is.data.frame(logit_prob)) logit_prob[[1]] else logit_prob

logit_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(logit_vec),
  model = "Logistic Regression"
)

roc_df <- bind_rows(knn_df, svm_df, rf_df, tree_df, logit_df)

roc_data = roc_df%>%
  group_by(model)%>%
  roc_curve(
    truth=Wildfire,
    .pred_Yes,
    event_level = 'second'
  )

ggplot(roc_data, aes(x=1-specificity, y=sensitivity, color=model)) +
  geom_line(size=1.2)+
  geom_abline(linetype='dashed', color ='gray50') +
  labs(
    title = 'ROC Curve Comparison Across Models (SMOTE-balanced)',
    x='False Positive Rate',
    y='True Positive Rate',
    color='Model') +
  theme_minimal(base_size=14)

```

The ROC curves show that Random Forest achieves the strongest overall ranking performance, while SVM and KNN exhibit more limited separation from the diagonal. The decision tree demonstrates competitive ranking ability without relying on ensemble complexity. Although ROC performance alone does not determine the final model choice, this comparison supports the decision tree as a balanced model when considered alongside recall, F1-score, and interpretability.

### 4.2.3. Feature Importance Interpretation(Decision Tree)

```{r}
# Variable importance
imp_vec <- tree_pruned$variable.importance

imp_df <- data.frame(
  Feature = names(imp_vec),
  Importance = as.numeric(imp_vec),
  row.names = NULL
) %>%
  dplyr::arrange(dplyr::desc(Importance))

imp_df %>%
  ggplot2::ggplot(ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Decision Tree Feature Importance",
    x = "Feature",
    y = "rpart variable.importance"
  ) +
  ggplot2::theme_minimal(base_size = 13)

```

The feature importance results indicate that variables related to atmospheric dryness, temperature, and fuel moisture play a central role in the decision tree’s wildfire predictions. These variables contribute most to the model’s split decisions, suggesting that climate-driven dryness conditions are key drivers of wildfire occurrence. The clear ranking of features also enhances interpretability, supporting the selection of the decision tree as the final model.

These results should be interpreted as model-specific importance rather than causal effects.

## 4.3 Limitations of the Study

- Despite improvements with SMOTE, the models still struggled to achieve high precision, indicating difficulty in correctly distinguishing non-wildfire events from actual fires.

- The dataset contains limited feature diversity, relying primarily on climate variables, without incorporating vegetation dryness, human activity, or land cover features.

- The temporal split (2018 train / 2019 test) reflects real-world conditions but introduces year-to-year variability that may reduce generalizability.

## 4.4 Future Works

- Add more features: vegetation dryness, land cover, human activity

- Explore advanced imbalance handling 

- Test stronger models: XGBoost, Gradient Boosting, ensembles

- Use spatial and temporal cross-validation for better generalization

# REFERENCES 
Abatzoglou, J. T., & Williams, A. P. (2016). Impact of anthropogenic climate change on wildfire across western US forests. Proceedings of the National Academy of Sciences, 113(42), 11770–11775. https://doi.org/10.1073/pnas.1607171113

Boulesteix, A.-L., Janitza, S., Kruppa, J., & König, I. R. (2018). Overview of machine learning methods for environmental data analysis. WIREs Data Mining and Knowledge Discovery, 8(6), e1240. https://doi.org/10.1002/widm.1240

Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953

Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46. https://doi.org/10.1177/001316446002000104.

Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010.

Flannigan, M. D., Krawchuk, M. A., de Groot, W. J., Wotton, B. M., & Gowman, L. M. (2009). Implications of changing climate for global wildland fire. International Journal of Wildland Fire, 18(5), 483–507. https://doi.org/10.1071/WF08187

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9), 1263–1284. https://doi.org/10.1109/TKDE.2008.239

Jolly, W. M., Cochrane, M. A., Freeborn, P. H., Holden, Z. A., Brown, T. J., Williamson, G. J., & Bowman, D. M. J. S. (2015). Climate-induced variations in global wildfire danger from 1979 to 2013. Nature Communications, 6, 7537. https://doi.org/10.1038/ncomms8537

Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer.

Kuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. R package documentation. https://www.tidymodels.org

McHugh, M. L. (2012). Interrater reliability: The kappa statistic. Biochemia Medica, 22(3), 276–282. https://doi.org/10.11613/BM.2012.031

R Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.r-project.org

Saito, T., & Rehmsmeier, M. (2015). The precision–recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PLOS ONE, 10(3), e0118432. https://doi.org/10.1371/journal.pone.0118432
