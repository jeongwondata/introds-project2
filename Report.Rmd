---
title: "Forecasting Wildfire Events from Climate Indicators: A Comparative ML Study"
author:
  Jeongwon Yoo
  Nithin Ravindra Reddy
  Simbanegavi Simbarashe  
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

library(caret)
library(corrplot)
library(dplyr)
library(ezids)
library(e1071)
library(ggplot2)
library(ROCR)
library(gridExtra)
library(lubridate)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)
library(viridis)
library(spdep)
library(spatialreg)
library(smotefamily)
library(sf)
library(themis)
library(tigris)
library(tidyr)
library(tidymodels)
library(treemapify)

knitr::opts_chunk$set(echo = TRUE, results = "markup", warning = FALSE, message = FALSE)
options(scientific = TRUE, digits = 3)


```

# CHAPTER 1: Introduction

## 1.0 Overview

Wildfires have become an increasingly severe environmental and societal challenge, driven by changing climate patterns, prolonged droughts, and expanding human activity. In recent years, wildfire incidents have intensified in both frequency and severity, resulting in substantial ecological damage, economic loss, and threats to human life. Accurate and timely prediction of wildfire occurrence is therefore a critical component of disaster preparedness and risk mitigation.

This study focuses on forecasting wildfire events using climate and spatial indicators derived from United States wildfire incidence data from 2018 to 2019. By applying multiple machine learning classification techniques, the research seeks to uncover relationships between climatic variables such as temperature, humidity, radiation, and precipitation and the likelihood of wildfire occurrence. A comparative modeling framework is used to evaluate how different algorithms perform under realistic conditions, particularly in the presence of extreme class imbalance between wildfire and non wildfire events.

## 1.1 Objectives
The primary objectives of this study are:

1).To develop and train multiple machine learning classification models including Logistic Regression, K Nearest Neighbors, Support Vector Machines, Decision Trees, and Random Forests to predict wildfire occurrence using climate and spatial features.

2).To evaluate and compare model performance using classification metrics such as accuracy, precision, recall, F1 score, Kappa, and ROC AUC, with particular emphasis on minority class detection.

3).To identify the most influential climate variables contributing to wildfire occurrence through model interpretation methods such as feature importance analysis.

## 1.2 Research Questions
This study is guided by the following research questions:

1).Can machine learning models accurately predict wildfire occurrence using climate and spatial indicators when trained on 2018 data and evaluated on 2019 data?

2).How do different machine learning algorithms compare in their ability to detect wildfire events under highly imbalanced data conditions?

3).Which climate related variables most strongly influence the likelihood of wildfire occurrence?

## 1.3 Significance of the Study

This research contributes to the literature on climate driven wildfire prediction by providing a comparative evaluation of multiple machine learning approaches using real world temporally split data. Unlike studies that rely primarily on overall accuracy, this work emphasizes performance metrics that are critical for detecting rare wildfire events.

From a practical standpoint, the findings offer insights into how climate indicators can support early warning systems and wildfire risk assessment. Identifying influential predictors and understanding model strengths can assist policymakers, environmental agencies, and emergency response teams in developing targeted prevention and monitoring strategies. Additionally, the study demonstrates a reproducible machine learning workflow in R that follows best practices for handling imbalanced environmental datasets.

## 1.4 Weaknesses of the Study

Despite its contributions, this study has several limitations.

First, the dataset primarily includes climate variables and lacks important contextual information. The absence of these factors may limit the models’ ability to fully capture the complex causes of wildfire events.

Second, the analysis relies on a single temporal split with 2018 used for training and 2019 used for testing. While this setup reflects realistic forecasting conditions, it may reduce generalizability due to year to year climate variability. Finally, although multiple algorithms were evaluated, extensive hyperparameter tuning was limited in order to maintain interpretability and manage computational complexity.

A major challenge in this study is the extreme class imbalance between wildfire and non wildfire observations. Wildfire events constitute a very small proportion of the dataset, allowing baseline models to achieve deceptively high accuracy by predominantly predicting the majority No class. As a result, these models perform poorly in identifying actual wildfire occurrences, leading to low recall and precision for the positive class.

To address this issue, imbalance handling techniques such as upsampling and the Synthetic Minority Oversampling Technique were applied to the data. While these approaches improved the models’ ability to detect wildfire events, they also increased the number of false positives, highlighting the trade off between recall and precision in rare event prediction. This limitation underscores the importance of careful metric selection and cautious interpretation of results when working with highly imbalanced classification problems.

# CHAPTER 2: EDA

Wildfire Yes/No Ratio

```{r}
data = read_csv("Fire_Incidence_Data 2018-2019.csv")

ratio_df <- data %>%
  count(Wildfire) %>%
  mutate(percent = n / sum(n) * 100)

ggplot(ratio_df, aes(x = Wildfire, y = percent, fill = Wildfire)) +
  geom_col(width = 0.6) +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Wildfire Class Distribution (%)",
    x = "Wildfire",
    y = "Percentage"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```
The class distribution shows a severe imbalance, with wildfire events occurring far less frequently than non-wildfire cases.

This imbalance explains why accuracy alone would be misleading, as a model could achieve high accuracy while failing to detect wildfire events. Addressing this imbalance is therefore essential for evaluating a model’s ability to identify wildfires rather than simply predicting the majority class.

Correlation Matrix
```{r}
num_vars <- data[, c("pr","rmax","rmin","sph","srad","tmmn")]

corr_mat <- cor(num_vars, method = "pearson")

corrplot(corr_mat, method = "number", type="upper",
         tl.col="black", title="Correlation Matrix (Climate Variables)")
```
The correlation matrix reveals moderate relationships among several climate variables, particularly those related to temperature and moisture.

While no single pair shows extreme correlation, these patterns suggest partial redundancy among predictors and highlight the risk of multicollinearity. As a result, model performance should be interpreted holistically rather than relying on individual coefficients.

# CHAPTER 3: Methodology

## 3.1 Preprocessing pipeline

```{r}
model = data %>%
  mutate(Wildfire = trimws(Wildfire),
         Wildfire = factor(Wildfire, levels = c('No','Yes')))%>%
  tidyr::drop_na()

table(data$Wildfire)
table(data$Year)
```

## 3.2 Train & Test split(SMOTE)

```{r}
train_2018 = model %>% filter(Year==2018) %>% select(-Year)
test_2019 = model %>% filter(Year==2019) %>% select(-Year)

smote = function(df,target='Wildfire',K=5,seed=2){
  df = as.data.frame(df)
  
  y = df[[target]]
  X = df[,setdiff(names(df), target), drop = FALSE]
  
   y_num = ifelse(y =='Yes',1,0)
   
   set.seed(seed)
   sm = smotefamily::SMOTE(X, y_num, K=K)
   
   out = sm$data
   colnames(out)[ncol(out)] = 'y_num'
   
   out[[target]] = factor(
     ifelse(out$y_num == 1, 'Yes','No'),
     levels = c('No','Yes')
   )
   
   out$y_num = NULL
   
   out = out[,c(setdiff(names(out), target), target)]
   
   return(out)
}

train_smote = smote(train_2018, target='Wildfire', K=5, seed=2)
test_smote = smote(test_2019, target='Wildfire',K=5,seed=2)

table(train_smote$Wildfire)
table(test_smote$Wildfire)
```

After applying SMOTE, the class counts become balanced, which prevents evaluation metrics from being dominated by the majority class. This allows model performance to be compared more directly in terms of wildfire detection ability and error trade-offs.

## 3.3 Modeling strategy

### 3.3.1 KNN

```{r}
fire_recipe_knn = recipe(Wildfire~., data = train_smote) %>% step_normalize(all_numeric_predictors())
```

1. Define the KNN Classification Model
```{r}
knn_model <- nearest_neighbor(
neighbors = 5,
weight_func = "rectangular",
dist_power = 2
) %>%
set_engine("kknn") %>%
set_mode("classification")

knn_model
```

2. Create the Workflow
```{r}
knn_wf <- workflow() %>%
add_recipe(fire_recipe_knn) %>%
add_model(knn_model)

knn_wf
```

3. Train the Model on 2018 Data
```{r}
knn_fit <- fit(knn_wf, data = train_smote)
```

4. Predict Wildfire Occurrence for 2019
```{r}
knn_predictions <- predict(knn_fit, test_smote) %>%
bind_cols(test_smote)

head(knn_predictions)
```

5. Confusion Matrix (Prediction Performance)
```{r}
conf_mat(
knn_predictions,
truth = Wildfire,
estimate = .pred_class
)
```

After applying upsampling to address class imbalance, the KNN model demonstrated an improved ability to detect wildfire events. Although some misclassification remains, the model shows a substantial improvement compared to the imbalanced version, which failed to detect any wildfire events. This result highlights the importance of addressing class imbalance in wildfire prediction tasks.

6. Classification Metrics (Accuracy, Recall, Precision, F1)

```{r metrics}
knn_metrics = yardstick::metric_set(
  yardstick::accuracy,
  yardstick::precision,
  yardstick::recall,
  yardstick::f_meas
)

knn_metrics(
  knn_predictions,
  truth = Wildfire,
  estimate = .pred_class,
  event_level = 'second'
)
```

While the model achieves relatively high precision, recall remains moderate, indicating that a substantial number of wildfire events are still misclassified as non-wildfire. This suggests that KNN tends to be conservative in issuing wildfire predictions, favoring precision over sensitivity, which limits its usefulness in high-risk detection scenarios.

7. Kappa
```{r kappa_wildfire}
metric_set(kap)(
  knn_predictions,
  truth = Wildfire,
  estimate = .pred_class,
  event_level = "second"   # "Yes" (Wildfire) is the positive class
)
```

Although the KNN model achieved high accuracy, the very low Kappa value confirms that its predictive power for wildfire occurrence is weak and only marginally better than random guessing.

8. ROC-AUC 

```{r knn-prob}
# Get predicted probabilities for both classes
knn_prob <- predict(knn_fit, test_smote, type = "prob") %>%
  bind_cols(test_smote)
```
```{r roc-plot, fig.width=7, fig.height=6}
# Create ROC object
roc_obj <- roc_curve(
  knn_prob,
  truth = Wildfire,
  .pred_Yes,
  event_level = "second"
)

# Plot ROC curve using ggplot
autoplot(roc_obj) +
  ggtitle("ROC Curve for KNN Wildfire Prediction (2019)") +
  theme_minimal()

```

The ROC curve for the KNN model does not show strong curvature toward the top-left corner, suggesting limited ranking ability across classification thresholds. This result implies that while KNN captures some signal from the climate variables, its overall discriminative power remains modest compared to more flexible models.

### 3.3.2 SVM

The SVM model is designed to find a decision boundary that maximizes separation between wildfire and non-wildfire cases in the feature space.

By allowing a flexible margin, the model can capture non-linear relationships among climate variables that simpler linear models may miss. This makes SVM a strong candidate for comparison in this wildfire classification task.

```{r}
svm_model <- svm(
  Wildfire ~ .,
  data = train_smote,
  kernel = "radial",
  probability = TRUE,
  scale = TRUE
)

# Predictions
svm_pred <- predict(svm_model, test_smote)
svm_cm <- table(Predicted = svm_pred, Actual = test_smote$Wildfire)
svm_cm
```

```{r}
# Accuracy
svm_accuracy <- sum(diag(svm_cm)) / sum(svm_cm)

# Metrics
TP_svm <- svm_cm["Yes", "Yes"]
FN_svm <- svm_cm["No",  "Yes"]
FP_svm <- svm_cm["Yes", "No"]

svm_precision <- TP_svm / (TP_svm + FP_svm)
svm_recall    <- TP_svm / (TP_svm + FN_svm)
svm_f1        <- 2 * (svm_precision * svm_recall) / (svm_precision + svm_recall)
 
c(
  accuracy  = svm_accuracy,
  precision = svm_precision,
  recall    = svm_recall,
  F1        = svm_f1
)
```

The SVM model achieves moderate improvements in recall and F1-score compared to KNN, indicating better sensitivity to wildfire events under SMOTE-balanced conditions. However, a substantial number of wildfire cases are still misclassified, suggesting that the model does not fully resolve the detection challenge posed by highly imbalanced wildfire data


```{r}
# Probability prediction
svm_prob <- attr(
  predict(svm_model, test_smote, probability = TRUE),
  "probabilities"
)[, "Yes"]

# ROCR performance objects
pred_svm <- prediction(svm_prob, test_smote$Wildfire)
perf_svm <- performance(pred_svm, "tpr", "fpr")

# Plot ROC curve
plot(perf_svm, col = "red", lwd = 2, main = "SVM ROC Curve")
abline(0, 1, lty = 2, col = "gray")

# Compute AUC
svm_auc <- performance(pred_svm, "auc")@y.values[[1]]
svm_auc
```

The ROC curve for the SVM model is smoother than that of KNN, reflecting more stable probability ranking across thresholds. However, the overall separation from the diagonal remains limited, and the curve does not indicate a dramatic improvement in discriminative power. This suggests that while SVM captures additional non-linear structure in the data, its overall ranking performance is only marginally better than KNN.

### 3.3.3 Random Forest

```{r}
# Random Forest on SMOTE balanced data

rf_model <- randomForest(
  Wildfire ~ .,
  data = train_smote,
  ntree = 500,
  importance = TRUE
)

# Predictions
rf_pred <- predict(rf_model, test_smote)
rf_cm <- table(Predicted = rf_pred, Actual = test_smote$Wildfire)
rf_cm
```


```{r}
# Metrics
rf_accuracy <- sum(diag(rf_cm)) / sum(rf_cm)

TP <- rf_cm["Yes", "Yes"]
FN <- rf_cm["No",  "Yes"]
FP <- rf_cm["Yes", "No"]

rf_precision <- TP / (TP + FP)
rf_recall    <- TP / (TP + FN)
rf_f1        <- 2 * (rf_precision * rf_recall) / (rf_precision + rf_recall)

c(accuracy = rf_accuracy,
  precision = rf_precision,
  recall    = rf_recall,
  F1        = rf_f1)

```


```{r}
# ROC curve
rf_prob <- predict(rf_model, newdata=test_smote, type = "prob")[,"Yes"]

pred <- prediction(rf_prob, test_smote$Wildfire)
perf <- performance(pred, "tpr", "fpr")

# AUC
auc_perf <- performance(pred, "auc")
rf_auc <- auc_perf@y.values[[1]]
rf_auc


plot(perf, col = "blue", lwd = 2, main = "Random Forest ROC Curve")
abline(0, 1, lty = 2, col = "gray")
```

The Random Forest ROC curve shows strong performance, with the curve approaching the upper-left corner of the plot. The high AUC value indicates excellent overall ranking ability, meaning the model is effective at assigning higher probabilities to wildfire events than non-wildfire cases. Despite this strong performance, the confusion matrix reveals that recall remains constrained, highlighting a trade-off between ranking performance and classification sensitivity under the chosen threshold.

### 3.3.4 Decision Tree

```{r}
tree_smote = rpart(Wildfire ~ .,
                   data = train_smote,
                   method = "class",
                   control = rpart.control(minsplit = 50, cp = 0.001))

test_smote$tree_pred_smote = predict(tree_smote,
                                    newdata = test_smote,
                                    type = "class")

cm_smote = table(Predicted = test_smote$tree_pred_smote,
                 Actual= test_smote$Wildfire)
cm_smote
```

```{r}
TP_s = cm_smote["Yes", "Yes"]
FP_s = cm_smote["Yes", "No"]
FN_s = cm_smote["No",  "Yes"]
TN_s = cm_smote["No",  "No"]

accuracy_s  = (TP_s + TN_s) / sum(cm_smote)
precision_s = TP_s / (TP_s + FP_s)
recall_s= TP_s / (TP_s + FN_s)
f1_s <- 2 * precision_s * recall_s / (precision_s + recall_s)

accuracy_s
precision_s
recall_s
f1_s

```

The decision tree model improves interpretability by explicitly modeling decision rules based on climate variables. Compared to logistic regression, the tree demonstrates improved recall, indicating better sensitivity to wildfire events. However, this gain is accompanied by increased false positives, reflecting a trade-off between detection and over-alerting.

smote data has a high risk of noise
Pruning tree
```{r}
printcp(tree_smote)

best_cp = tree_smote$cptable[which.min(tree_smote$cptable[, "xerror"]), "CP"]

best_cp

tree_pruned = rpart::prune(tree_smote, cp = best_cp)

rpart.plot(tree_pruned,type = 1,extra = 1,cex = 0.4,faclen = 3)

test_smote$tree_pred_pruned <- predict(
  tree_pruned,
  newdata = test_smote,
  type = "class"
)

cm_pruned <- table(
  Predicted = test_smote$tree_pred_pruned,
  Actual    = test_smote$Wildfire
)
cm_pruned

TP_p <- cm_pruned["Yes", "Yes"]
FP_p <- cm_pruned["Yes", "No"]
FN_p <- cm_pruned["No",  "Yes"]
TN_p <- cm_pruned["No",  "No"]

accuracy_p  <- (TP_p + TN_p) / sum(cm_pruned)
precision_p <- TP_p / (TP_p + FP_p)
recall_p    <- TP_p / (TP_p + FN_p)
f1_p <- 2 * precision_p * recall_p / (precision_p + recall_p)

accuracy_p
precision_p
recall_p
f1_p

tree_prob <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]

tree_prob <- data.frame(
  .pred_Yes = tree_prob,
  model = "Decision Tree"
)
```

Pruning reduces model complexity and helps prevent overfitting to the training data. The pruned tree shows more balanced performance, with slightly reduced recall but improved generalization. This suggests that controlling model complexity leads to more stable wildfire predictions under balanced evaluation conditions.

```{r}
# Create ROC object
tree_prob_yes <- predict(tree_pruned, test_smote, type = "prob")[, "Yes"]
tree_prob_yes <- as.numeric(tree_prob_yes)

tree_df <- data.frame(
  Wildfire  = test_smote$Wildfire,
  .pred_Yes = tree_prob_yes
)

names(tree_df)

roc_dt <- roc_curve(tree_df,
                    truth = Wildfire,
                    .pred_Yes,
                    event_level = 'second')

# Plot ROC curve using ggplot
autoplot(roc_dt) +
  ggtitle("ROC Curve for Pruned Decision Tree (2019)") +
  theme_minimal()
```

### 3.3.5 Logistic Regression

logistic regression with SMOTE

```{r}
logit_smote_model <- glm(
  Wildfire ~ .,
  data   = train_smote,
  family = binomial
)

summary(logit_smote_model)
```
The logistic regression model provides a transparent baseline for wildfire prediction. While it captures general trends in the data, its recall remains limited, indicating difficulty in identifying wildfire events under a linear decision boundary. This result suggests that more flexible, non-linear models may be better suited for capturing complex wildfire–climate relationships..

```{r}
test_smote$prob_yes_smote <- predict(
  logit_smote_model,
  newdata = test_smote,
  type   = "response"
)

eval_cutoff <- function(prob, actual, cutoff) {
  pred <- ifelse(prob >= cutoff, "Yes", "No")
  pred <- factor(pred, levels = c("No", "Yes"))
  
  cm <- table(Predicted = pred, Actual = actual)
  print(cm)
  
  TP_l <- cm["Yes", "Yes"]
  FP_l <- cm["Yes", "No"]
  FN_l <- cm["No",  "Yes"]
  TN_l <- cm["No",  "No"]
  
  accuracy_l  <- (TP_l + TN_l) / sum(cm)
  precision_l <- TP_l / (TP_l + FP_l)
  recall_l    <- TP_l / (TP_l + FN_l)
  f1_l        <- 2 * precision_l * recall_l / (precision_l + recall_l)
  
  out <- c(
    cutoff    = cutoff,
    accuracy  = accuracy_l,
    precision = precision_l,
    recall    = recall_l,
    f1        = f1_l
  )
  
  return(out)
}

cutoffs <- c(0.1, 0.2, 0.3, 0.5)

results_logit_smote <- t(
  sapply(cutoffs, function(c) {
    eval_cutoff(
      prob   = test_smote$prob_yes_smote,
      actual = test_smote$Wildfire,
      cutoff = c
    )
  })
)

results_logit_smote <- as.data.frame(results_logit_smote)
results_logit_smote[] <- lapply(results_logit_smote, as.numeric)

knitr::kable(results_logit_smote, digits = 4)

logit_prob <- predict(logit_smote_model, test_smote, type = "response")

logit_prob <- data.frame(
  .pred_Yes = logit_prob,
  model = "Logistic Regression"
)

```
The confusion matrix shows that a substantial portion of wildfire cases are misclassified as non-wildfire, resulting in lower recall. Although false positives are relatively controlled, the cost of missed wildfire detections is high in this context. Therefore, recall is prioritized over overall accuracy for model evaluation.

```{r}
# Create ROC object

logit_prob_yes <- predict(
  logit_smote_model,
  newdata = test_smote,
  type = "response"
)

logit_df = data.frame(Wildfire = test_smote$Wildfire,
                      .pred_Yes = logit_prob_yes)

roc_logit <- roc_curve(
  logit_df,
  truth = Wildfire,
  .pred_Yes,
  event_level = "second"
)

# Plot ROC curve using ggplot
autoplot(roc_logit) +
  ggtitle("ROC Curve for Logistic Regression (2019)") +
  theme_minimal()
```
# CHAPTER 4: Conclusion & Future Works

## 4.1 Summary of Findings

Our analysis examined the relationship between climate variables and wildfire occurrence.
The primary challenge was the extreme class imbalance, which caused most baseline models to fail at detecting wildfire events.
SMOTE-based preprocessing improved the model's ability to learn the minority class, allowing us to evaluate which algorithms performed best in predicting wildfire events.

## 4.2 Best Performing Model and Justification

Comparing Recall & F1 Score
```{r}
logit_prob <- predict(logit_smote_model, newdata = test_smote, type = "response")
logit_prob <- as.numeric(logit_prob)

knn_pred_class = predict(knn_fit, test_smote)$.pred_class
svm_pred_class = predict(svm_model, test_smote)
rf_pred_class = predict(rf_model, newdata=test_smote, type='class')
dt_pred_class = predict(tree_pruned, newdata = test_smote, type='class')
logit_pred_class = factor(ifelse(logit_prob >= 0.5, 'Yes','No'), levels = c('No','Yes'))

fix_levels = function(x) factor(x, levels = c('No','Yes'))
truth_fix = fix_levels(test_smote$Wildfire)

knn_pred_class = fix_levels(knn_pred_class)
svm_pred_class = fix_levels(svm_pred_class)
rf_pred_class = fix_levels(rf_pred_class)
dt_pred_class = fix_levels(dt_pred_class)
logit_pred_class = fix_levels(logit_pred_class)

metric_c = yardstick::metric_set(yardstick::recall,yardstick::f_meas)
metrics_df = bind_rows(
  tibble(model='KNN', truth = truth_fix, estimate = knn_pred_class),
  tibble(model='SVM', truth = truth_fix, estimate = svm_pred_class),
  tibble(model='Random Forest', truth = truth_fix, estimate = rf_pred_class),
  tibble(model='Decision Tree', truth = truth_fix, estimate = dt_pred_class),
  tibble(model='Logistic Regression', truth = truth_fix, estimate = logit_pred_class)
)%>%
  group_by(model)%>%
  metric_c(truth=truth, estimate = estimate, event_level = 'second')%>%
  ungroup()%>%
  select(model, .metric,.estimate)

ggplot(metrics_df, aes(x=model, y=.estimate, fill=.metric)) + geom_col(position = position_dodge(width=0.8), width =0.7) + coord_flip()+ labs(
  title = 'Comparing Recall and F1 Score Across Models(SMOTE-balanced)',
  x='Models',
  y='Score',
  fil='Metric'
)+
  theme_minimal(base_size=14) +
  theme(axis.test.x = element_text(angle=25, hjust=1))
```


Comparing AUC-ROC
```{r}
rm(roc_df)
gc()

# KNN
knn_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = predict(knn_fit, test_smote, type = "prob")$.pred_Yes,
  model = "KNN"
)

# SVM
svm_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(svm_prob),
  model = "SVM"
)

# Random Forest
rf_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(rf_prob),
  model = "Random Forest"
)

# Decision Tree
tree_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(tree_prob$.pred_Yes),
  model = "Decision Tree"
)

# Logistic
logit_vec <- if (is.data.frame(logit_prob)) logit_prob[[1]] else logit_prob

logit_df <- tibble(
  Wildfire = test_smote$Wildfire,
  .pred_Yes = as.numeric(logit_vec),
  model = "Logistic Regression"
)

roc_df <- bind_rows(knn_df, svm_df, rf_df, tree_df, logit_df)

roc_data = roc_df%>%
  group_by(model)%>%
  roc_curve(
    truth=Wildfire,
    .pred_Yes,
    event_level = 'second'
  )

ggplot(roc_data, aes(x=1-specificity, y=sensitivity, color=model)) +
  geom_line(size=1.2)+
  geom_abline(linetype='dashed', color ='gray50') +
  labs(
    title = 'ROC Curve Comparison Across Models (SMOTE-balanced)',
    x='False Positive Rate',
    y='True Positive Rate',
    color='Model') +
  theme_minimal(base_size=14)

```


```{r}
# Variable importance
imp_vec <- tree_pruned$variable.importance

imp_df <- data.frame(
  Feature = names(imp_vec),
  Importance = as.numeric(imp_vec),
  row.names = NULL
) %>%
  dplyr::arrange(dplyr::desc(Importance))

imp_df %>%
  ggplot2::ggplot(ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Decision Tree Feature Importance",
    x = "Feature",
    y = "rpart variable.importance"
  ) +
  ggplot2::theme_minimal(base_size = 13)

```

## 4.3 Limitations of the Study

Despite improvements with SMOTE, the models still struggled to achieve high precision, indicating difficulty in correctly distinguishing non-wildfire events from actual fires.

In addition, the dataset contains limited feature diversity, relying primarily on climate variables, without incorporating vegetation dryness, human activity, or land cover features.

Finally, the temporal split (2018 train / 2019 test) reflects real-world conditions but introduces year-to-year variability that may reduce generalizability.

## 4.4 Future Works
Add more features: vegetation dryness, land cover, human activity

Explore advanced imbalance handling 

Test stronger models: XGBoost, Gradient Boosting, ensembles

Use spatial and temporal cross-validation for better generalization

# REFERENCES 
Abatzoglou, J. T., & Williams, A. P. (2016). Impact of anthropogenic climate change on wildfire across western US forests. Proceedings of the National Academy of Sciences, 113(42), 11770–11775. https://doi.org/10.1073/pnas.1607171113

Boulesteix, A.-L., Janitza, S., Kruppa, J., & König, I. R. (2018). Overview of machine learning methods for environmental data analysis. WIREs Data Mining and Knowledge Discovery, 8(6), e1240. https://doi.org/10.1002/widm.1240

Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953

Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46. https://doi.org/10.1177/001316446002000104.

Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010.

Flannigan, M. D., Krawchuk, M. A., de Groot, W. J., Wotton, B. M., & Gowman, L. M. (2009). Implications of changing climate for global wildland fire. International Journal of Wildland Fire, 18(5), 483–507. https://doi.org/10.1071/WF08187

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9), 1263–1284. https://doi.org/10.1109/TKDE.2008.239

Jolly, W. M., Cochrane, M. A., Freeborn, P. H., Holden, Z. A., Brown, T. J., Williamson, G. J., & Bowman, D. M. J. S. (2015). Climate-induced variations in global wildfire danger from 1979 to 2013. Nature Communications, 6, 7537. https://doi.org/10.1038/ncomms8537

Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer.

Kuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. R package documentation. https://www.tidymodels.org

McHugh, M. L. (2012). Interrater reliability: The kappa statistic. Biochemia Medica, 22(3), 276–282. https://doi.org/10.11613/BM.2012.031

R Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.r-project.org

Saito, T., & Rehmsmeier, M. (2015). The precision–recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PLOS ONE, 10(3), e0118432. https://doi.org/10.1371/journal.pone.0118432
